{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc8efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Base class for a node in the expression tree.\"\"\"\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "\n",
    "    def evaluate(self, X, functions):\n",
    "        \"\"\"Evaluates the expression tree rooted at this node.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement evaluate\")\n",
    "\n",
    "class FunctionNode(Node):\n",
    "    \"\"\"Represents an internal node with a function.\"\"\"\n",
    "    def __init__(self, function_name, function_callable):\n",
    "        super().__init__(function_name)\n",
    "        self.function_callable = function_callable # Store the actual function\n",
    "\n",
    "    def evaluate(self, X, functions):\n",
    "        \"\"\"Evaluates the function node by evaluating its children and applying the function.\"\"\"\n",
    "        args = [child.evaluate(X, functions) for child in self.children]\n",
    "        try:\n",
    "            # Ensure all arguments have compatible shapes (broadcast if needed)\n",
    "            # This is a simple attempt; more robust handling might be needed\n",
    "            if args and any(isinstance(arg, np.ndarray) for arg in args):\n",
    "                 # Find the shape of the first array argument\n",
    "                array_shape = next((arg.shape for arg in args if isinstance(arg, np.ndarray)), None)\n",
    "\n",
    "                # Broadcast scalar arguments to match the array shape\n",
    "                args = [np.full(array_shape, arg) if not isinstance(arg, np.ndarray) else arg for arg in args]\n",
    "\n",
    "\n",
    "            result = self.function_callable(*args)\n",
    "            # Handle potential NaN or Inf results\n",
    "            result = np.nan_to_num(result, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            # Return a column of zeros in case of evaluation error\n",
    "            return np.zeros(X.shape[0])\n",
    "\n",
    "\n",
    "class TerminalNode(Node):\n",
    "    \"\"\"Represents a leaf node with a feature index.\"\"\"\n",
    "    def __init__(self, feature_index):\n",
    "        super().__init__(feature_index)\n",
    "\n",
    "    def evaluate(self, X, functions):\n",
    "        \"\"\"Evaluates the terminal node by returning the corresponding feature column.\"\"\"\n",
    "        try:\n",
    "            return X[:, self.value]\n",
    "        except IndexError:\n",
    "            # Handle cases where feature_index is out of bounds\n",
    "            return np.zeros(X.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1320b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "import time\n",
    "# Assume Node, FunctionNode, TerminalNode classes are already defined from the previous step\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class EvolutionaryOptimizerGP(LinearRegression):\n",
    "    \"\"\"\n",
    "    Optimizador evolutivo para transformación de datasets usando GP y GA.\n",
    "    Usa regresión lineal como modelo base para la función de fitness.\n",
    "    Soporta validación por hold-out o cross-validation.\n",
    "\n",
    "    Modificado para usar árboles de expresión para las transformaciones (GP).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxtime=1800, population_size=50, max_tree_depth=5,\n",
    "                 mutation_prob=0.15, crossover_prob=0.7, tournament_size=3,\n",
    "                 validation_method='cv', cv_folds=3, validation_size=0.2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        maxtime : int\n",
    "            Tiempo máximo de ejecución en segundos\n",
    "        population_size : int\n",
    "            Tamaño de la población\n",
    "        max_tree_depth : int\n",
    "            Profundidad máxima del árbol de expresión\n",
    "        mutation_prob : float\n",
    "            Probabilidad de mutación\n",
    "        crossover_prob : float\n",
    "            Probabilidad de cruce\n",
    "        tournament_size : int\n",
    "            Tamaño del torneo para selección\n",
    "        validation_method : str, 'cv' o 'holdout'\n",
    "            Método de validación: 'cv' para cross-validation, 'holdout' para división train/val\n",
    "        cv_folds : int\n",
    "            Número de folds para cross-validation (solo si validation_method='cv')\n",
    "        validation_size : float\n",
    "            Proporción del conjunto de validación (solo si validation_method='holdout')\n",
    "        \"\"\"\n",
    "        self.maxtime = maxtime\n",
    "        self.population_size = population_size\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.crossover_prob = crossover_prob\n",
    "        self.tournament_size = tournament_size\n",
    "        self.validation_method = validation_method\n",
    "        self.cv_folds = cv_folds\n",
    "        self.validation_size = validation_size\n",
    "\n",
    "        # Mejores soluciones encontradas\n",
    "        self.best_transformation_trees_ = None # Now stores a list of root nodes\n",
    "        self.best_selection_ = None\n",
    "        self.n_features_in_ = None\n",
    "        self.best_fitness_history_ = []\n",
    "\n",
    "        # Funciones disponibles para transformaciones (including arity)\n",
    "        self.functions = {\n",
    "            'add': (lambda x, y: x + y, 2),\n",
    "            'sub': (lambda x, y: x - y, 2),\n",
    "            'mul': (lambda x, y: x * y, 2),\n",
    "            'div': (lambda x, y: np.divide(x, y + 1e-10), 2),\n",
    "            'sqrt': (lambda x: np.sqrt(np.abs(x)), 1),\n",
    "            'square': (lambda x: np.square(x), 1),\n",
    "            'log': (lambda x: np.log(np.abs(x) + 1), 1),\n",
    "            'abs': (lambda x: np.abs(x), 1),\n",
    "        }\n",
    "        self.function_names = list(self.functions.keys())\n",
    "        self.terminals = None # Will be feature indices\n",
    "\n",
    "        # Linear Regression model for fitness evaluation\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Entrena el optimizador usando algoritmos evolutivos (GP).\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Datos de entrada\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Variable objetivo\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Convertir a numpy array si es necesario\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.terminals = list(range(self.n_features_in_)) # Feature indices as terminals\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Tiempo máximo: {self.maxtime} segundos ({self.maxtime/60:.1f} minutos)\")\n",
    "        print(f\"Tamaño población: {self.population_size}\")\n",
    "        print(f\"Features originales: {self.n_features_in_}\")\n",
    "        print(f\"Profundidad máxima del árbol: {self.max_tree_depth}\")\n",
    "        print(f\"Método de validación: {self.validation_method.upper()}\")\n",
    "        if self.validation_method == 'cv':\n",
    "            print(f\"Cross-Validation con {self.cv_folds} folds\")\n",
    "        else:  # holdout\n",
    "            print(f\"Proporción de validación: {self.validation_size}\")\n",
    "\n",
    "\n",
    "        # Configurar validación según el método\n",
    "        if self.validation_method == 'holdout':\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=self.validation_size, random_state=42, shuffle=True\n",
    "            )\n",
    "            print(f\"Train: {X_train.shape[0]} muestras | Validation: {X_val.shape[0]} muestras\")\n",
    "            eval_data = (X_train, y_train, X_val, y_val)\n",
    "        else:  # cv\n",
    "            eval_data = (X, y)\n",
    "\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # Inicializar población (each individual is a list of transformation trees + selection)\n",
    "        population = self._initialize_population()\n",
    "\n",
    "        # Evaluar población inicial\n",
    "        if self.validation_method == 'holdout':\n",
    "            fitness_scores = [self._evaluate_fitness_holdout(ind, *eval_data)\n",
    "                            for ind in population]\n",
    "        else:\n",
    "            fitness_scores = [self._evaluate_fitness_cv(ind, *eval_data)\n",
    "                            for ind in population]\n",
    "\n",
    "        # Mejor individuo hasta el momento\n",
    "        best_idx = np.argmin(fitness_scores)\n",
    "        best_individual = population[best_idx] # No need to copy; trees are immutable structures (in this basic impl)\n",
    "        best_fitness = fitness_scores[best_idx]\n",
    "        self.best_fitness_history_ = [best_fitness]\n",
    "\n",
    "        generation = 0\n",
    "        last_improvement_gen = 0\n",
    "\n",
    "        print(f\"Generación 0 - Fitness inicial: {best_fitness:.4f}\")\n",
    "\n",
    "        # Loop evolutivo\n",
    "        while (time.time() - start_time) < self.maxtime:\n",
    "            generation += 1\n",
    "\n",
    "            # Nueva generación\n",
    "            new_population = []\n",
    "\n",
    "            # Elitismo: mantener el mejor\n",
    "            new_population.append(best_individual)\n",
    "\n",
    "            # Generar resto de la población\n",
    "            while len(new_population) < self.population_size:\n",
    "                # Selección por torneo\n",
    "                parent1 = self._tournament_selection(population, fitness_scores)\n",
    "                parent2 = self._tournament_selection(population, fitness_scores)\n",
    "\n",
    "                # Cruce\n",
    "                if random.random() < self.crossover_prob:\n",
    "                    child1_trees, child2_trees = self._crossover_trees(parent1['transformation_trees'], parent2['transformation_trees'])\n",
    "                    child1_selection, child2_selection = self._crossover_selection(parent1['selection'], parent2['selection'])\n",
    "                    child1 = {'transformation_trees': child1_trees, 'selection': child1_selection}\n",
    "                    child2 = {'transformation_trees': child2_trees, 'selection': child2_selection}\n",
    "                else:\n",
    "                    child1, child2 = parent1, parent2 # No need to copy\n",
    "\n",
    "                # Mutación\n",
    "                if random.random() < self.mutation_prob:\n",
    "                    child1 = self._mutate(child1)\n",
    "                if random.random() < self.mutation_prob:\n",
    "                    child2 = self._mutate(child2)\n",
    "\n",
    "                new_population.extend([child1, child2])\n",
    "\n",
    "            # Limitar al tamaño de población\n",
    "            new_population = new_population[:self.population_size]\n",
    "\n",
    "            # Evaluar nueva población\n",
    "            population = new_population\n",
    "            if self.validation_method == 'holdout':\n",
    "                fitness_scores = [self._evaluate_fitness_holdout(ind, *eval_data)\n",
    "                                for ind in population]\n",
    "            else:\n",
    "                fitness_scores = [self._evaluate_fitness_cv(ind, *eval_data)\n",
    "                                for ind in population]\n",
    "\n",
    "            # Actualizar mejor individuo\n",
    "            current_best_idx = np.argmin(fitness_scores)\n",
    "            if fitness_scores[current_best_idx] < best_fitness:\n",
    "                best_fitness = fitness_scores[current_best_idx]\n",
    "                best_individual = population[current_best_idx]\n",
    "                last_improvement_gen = generation\n",
    "                print(f\"Gen {generation} - MEJORA! Fitness: {best_fitness:.4f} \" +\n",
    "                      f\"(Tiempo: {(time.time()-start_time)/60:.1f}min)\")\n",
    "\n",
    "            self.best_fitness_history_.append(best_fitness)\n",
    "\n",
    "            # Log progreso cada 50 generaciones\n",
    "            if generation % 50 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                progress = (elapsed / self.maxtime) * 100\n",
    "                print(f\"Gen {generation} | Fitness: {best_fitness:.4f} | \" +\n",
    "                      f\"Tiempo: {elapsed/60:.1f}min ({progress:.1f}%) | \" +\n",
    "                      f\"Última mejora: gen {last_improvement_gen}\")\n",
    "\n",
    "\n",
    "        # Guardar mejor solución\n",
    "        self.best_transformation_trees_ = best_individual['transformation_trees']\n",
    "        self.best_selection_ = best_individual['selection']\n",
    "\n",
    "        elapsed_total = time.time() - start_time\n",
    "        n_features_generated = len(self.best_transformation_trees_)\n",
    "        total_features = self.n_features_in_ + n_features_generated\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"OPTIMIZACIÓN COMPLETADA (GP)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Generaciones totales: {generation}\")\n",
    "        print(f\"Tiempo total: {elapsed_total/60:.2f} minutos\")\n",
    "        print(f\"Mejor fitness: {best_fitness:.4f}\")\n",
    "        print(f\"Features originales: {self.n_features_in_}\")\n",
    "        print(f\"Features generadas: {n_features_generated}\")\n",
    "        print(f\"Features totales: {total_features}\")\n",
    "        print(f\"Features seleccionadas: {np.sum(self.best_selection_)}\")\n",
    "        if total_features > 0:\n",
    "            print(f\"Tasa de reducción: {(1 - np.sum(self.best_selection_)/total_features)*100:.1f}%\")\n",
    "        else:\n",
    "             print(\"Tasa de reducción: N/A (no features available)\")\n",
    "\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforma los datos usando las transformaciones aprendidas (árboles)\n",
    "        y la selección de features.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Datos a transformar\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : array, shape (n_samples, n_selected_features)\n",
    "            Datos transformados\n",
    "        \"\"\"\n",
    "        if self.best_transformation_trees_ is None or self.best_selection_ is None:\n",
    "            raise ValueError(\"El optimizador no ha sido entrenado. Llama a fit() primero.\")\n",
    "\n",
    "        # Convertir a numpy array si es necesario\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "\n",
    "        # Evaluate the transformation trees\n",
    "        generated_features = []\n",
    "        for tree_root in self.best_transformation_trees_:\n",
    "            generated_features.append(tree_root.evaluate(X, self.functions).reshape(-1, 1))\n",
    "\n",
    "        # Combine original and generated features\n",
    "        X_combined = X.copy()\n",
    "        if generated_features:\n",
    "            X_combined = np.hstack([X_combined] + generated_features)\n",
    "\n",
    "        # Apply feature selection\n",
    "        if X_combined.shape[1] != len(self.best_selection_):\n",
    "             raise ValueError(f\"Mismatch between number of combined features ({X_combined.shape[1]}) and selection mask size ({len(self.best_selection_)}). This should not happen.\")\n",
    "\n",
    "        X_selected = X_combined[:, self.best_selection_]\n",
    "\n",
    "        return X_selected\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fits the optimizer and then transforms the data.\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "    def _initialize_population(self):\n",
    "        \"\"\"Inicializa la población con individuos aleatorios (árboles + selección).\"\"\"\n",
    "        population = []\n",
    "\n",
    "        for _ in range(self.population_size):\n",
    "            # Each individual has a list of transformation trees\n",
    "            transformation_trees = [self._create_random_tree(self.max_tree_depth)\n",
    "                                    for _ in range(random.randint(1, self.n_features_in_))] # Create 1 to n_features_in_ trees\n",
    "\n",
    "            # Initialize selection mask (original features + generated features)\n",
    "            initial_selection_size = self.n_features_in_ + len(transformation_trees)\n",
    "            selection = np.ones(initial_selection_size, dtype=bool)\n",
    "\n",
    "            # Ensure at least one original feature is selected initially\n",
    "            if self.n_features_in_ > 0 and not np.any(selection[:self.n_features_in_]):\n",
    "                 selection[random.randint(0, self.n_features_in_ - 1)] = True\n",
    "\n",
    "\n",
    "            individual = {\n",
    "                'transformation_trees': transformation_trees,\n",
    "                'selection': selection\n",
    "            }\n",
    "            population.append(individual)\n",
    "\n",
    "        return population\n",
    "\n",
    "    def _create_random_tree(self, max_depth, current_depth=0):\n",
    "        \"\"\"Creates a random expression tree using the 'Grow' method.\"\"\"\n",
    "        if current_depth >= max_depth or (current_depth > 0 and random.random() < 0.5):\n",
    "            # Create a terminal node (feature index)\n",
    "            feature_index = random.choice(self.terminals)\n",
    "            return TerminalNode(feature_index)\n",
    "        else:\n",
    "            # Create a function node\n",
    "            func_name = random.choice(self.function_names)\n",
    "            func_callable, arity = self.functions[func_name]\n",
    "            function_node = FunctionNode(func_name, func_callable)\n",
    "\n",
    "            # Create children\n",
    "            for _ in range(arity):\n",
    "                function_node.children.append(\n",
    "                    self._create_random_tree(max_depth, current_depth + 1)\n",
    "                )\n",
    "            return function_node\n",
    "\n",
    "    def _evaluate_individual(self, individual, X):\n",
    "        \"\"\"Evaluates an individual's transformations and returns the combined dataset.\"\"\"\n",
    "        original_features = X.copy()\n",
    "        generated_features = []\n",
    "\n",
    "        for tree_root in individual['transformation_trees']:\n",
    "            generated_features.append(tree_root.evaluate(X, self.functions).reshape(-1, 1))\n",
    "\n",
    "        # Combine original and generated features\n",
    "        X_combined = original_features\n",
    "        if generated_features:\n",
    "            X_combined = np.hstack([X_combined] + generated_features)\n",
    "\n",
    "        # Apply selection\n",
    "        X_selected = X_combined[:, individual['selection']]\n",
    "\n",
    "        return X_selected\n",
    "\n",
    "\n",
    "    def _evaluate_fitness_holdout(self, individual, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Evalúa el fitness usando validación hold-out.\n",
    "        Entrena en X_train y evalúa en X_val.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Apply transformations and selection\n",
    "            X_train_selected = self._evaluate_individual(individual, X_train)\n",
    "            X_val_selected = self._evaluate_individual(individual, X_val)\n",
    "\n",
    "            if X_train_selected.shape[1] == 0 or X_val_selected.shape[1] == 0:\n",
    "                return 1e10 # Penalize individuals with no selected features\n",
    "\n",
    "            if np.any(np.std(X_train_selected, axis=0) < 1e-10):\n",
    "                return 1e10 # Penalize individuals with zero-variance features\n",
    "\n",
    "\n",
    "            # Entrenar modelo en TRAIN\n",
    "            model = LinearRegression() # Create a new model each time to avoid state issues\n",
    "            model.fit(X_train_selected, y_train)\n",
    "\n",
    "            # Evaluar en VALIDATION\n",
    "            y_val_pred = model.predict(X_val_selected)\n",
    "            fitness = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "            # Penalización por complejidad (number of selected features)\n",
    "            complexity_penalty = 0.01 * X_train_selected.shape[1]\n",
    "\n",
    "            return fitness + complexity_penalty\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(f\"Error during holdout fitness evaluation: {e}\") # For debugging\n",
    "            return 1e10 # Assign a high fitness in case of errors\n",
    "\n",
    "    def _evaluate_fitness_cv(self, individual, X, y):\n",
    "        \"\"\"\n",
    "        Evalúa el fitness usando cross-validation.\n",
    "        Más robusto pero más lento que hold-out.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Apply transformations and selection\n",
    "            X_selected = self._evaluate_individual(individual, X)\n",
    "\n",
    "            if X_selected.shape[1] == 0:\n",
    "                return 1e10 # Penalize individuals with no selected features\n",
    "\n",
    "            if np.any(np.std(X_selected, axis=0) < 1e-10):\n",
    "                return 1e10 # Penalize individuals with zero-variance features\n",
    "\n",
    "            # Cross-validation\n",
    "            model = LinearRegression() # Create a new model each time\n",
    "            scores = cross_val_score(\n",
    "                model, X_selected, y,\n",
    "                cv=self.cv_folds,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=1 # Use 1 job to avoid issues with multiprocessing and tree evaluation\n",
    "            )\n",
    "\n",
    "            fitness = -scores.mean()\n",
    "\n",
    "            # Penalización por complejidad (number of selected features)\n",
    "            complexity_penalty = 0.01 * X_selected.shape[1]\n",
    "\n",
    "\n",
    "            return fitness + complexity_penalty\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(f\"Error during CV fitness evaluation: {e}\") # For debugging\n",
    "            return 1e10 # Assign a high fitness in case of errors\n",
    "\n",
    "\n",
    "    def _tournament_selection(self, population, fitness_scores):\n",
    "        \"\"\"Selección por torneo.\"\"\"\n",
    "        tournament_idx = random.sample(range(len(population)), self.tournament_size)\n",
    "        tournament_fitness = [fitness_scores[i] for i in tournament_idx]\n",
    "        winner_idx = tournament_idx[np.argmin(tournament_fitness)]\n",
    "        return population[winner_idx] # Return reference, not copy\n",
    "\n",
    "\n",
    "    def _get_random_subtree(self, node):\n",
    "        \"\"\"Recursively gets a random subtree from the given node.\"\"\"\n",
    "        nodes = []\n",
    "        def collect_nodes(n):\n",
    "            nodes.append(n)\n",
    "            for child in n.children:\n",
    "                collect_nodes(child)\n",
    "        collect_nodes(node)\n",
    "        return random.choice(nodes)\n",
    "\n",
    "    def _replace_subtree(self, root, old_subtree, new_subtree):\n",
    "        \"\"\"Replaces old_subtree with new_subtree in the tree rooted at root.\"\"\"\n",
    "        if root is old_subtree:\n",
    "            return new_subtree\n",
    "\n",
    "        for i, child in enumerate(root.children):\n",
    "            if child is old_subtree:\n",
    "                root.children[i] = new_subtree\n",
    "                return root\n",
    "            else:\n",
    "                # Check if old_subtree is in the child's subtree\n",
    "                replaced_child = self._replace_subtree(child, old_subtree, new_subtree)\n",
    "                if replaced_child is not child:\n",
    "                    root.children[i] = replaced_child\n",
    "                    return root\n",
    "        return root # Should not reach here if old_subtree is in the tree\n",
    "\n",
    "\n",
    "    def _crossover_trees(self, parent1_trees, parent2_trees):\n",
    "        \"\"\"Performs crossover on the list of transformation trees.\"\"\"\n",
    "        child1_trees = parent1_trees[:] # Create a copy of the list\n",
    "        child2_trees = parent2_trees[:]\n",
    "\n",
    "        if not child1_trees or not child2_trees:\n",
    "             return child1_trees, child2_trees\n",
    "\n",
    "        # Choose a random tree from each parent's list\n",
    "        tree1_idx = random.randint(0, len(child1_trees) - 1)\n",
    "        tree2_idx = random.randint(0, len(child2_trees) - 1)\n",
    "\n",
    "        tree1_root = child1_trees[tree1_idx]\n",
    "        tree2_root = child2_trees[tree2_idx]\n",
    "\n",
    "        # Choose random subtrees for crossover\n",
    "        subtree1 = self._get_random_subtree(tree1_root)\n",
    "        subtree2 = self._get_random_subtree(tree2_root)\n",
    "\n",
    "        # Perform crossover by swapping subtrees\n",
    "        new_tree1_root = self._replace_subtree(tree1_root, subtree1, subtree2)\n",
    "        new_tree2_root = self._replace_subtree(tree2_root, subtree2, subtree1)\n",
    "\n",
    "        # Update the trees in the children's lists\n",
    "        child1_trees[tree1_idx] = new_tree1_root\n",
    "        child2_trees[tree2_idx] = new_tree2_root\n",
    "\n",
    "        return child1_trees, child2_trees\n",
    "\n",
    "    def _crossover_selection(self, parent1_selection, parent2_selection):\n",
    "        \"\"\"Performs crossover on the selection masks.\"\"\"\n",
    "        # Assuming selection masks have potentially different lengths\n",
    "        min_len = min(len(parent1_selection), len(parent2_selection))\n",
    "        crossover_point = random.randint(1, min_len - 1)\n",
    "\n",
    "        child1_selection = np.hstack((parent1_selection[:crossover_point], parent2_selection[crossover_point:]))\n",
    "        child2_selection = np.hstack((parent2_selection[:crossover_point], parent1_selection[crossover_point:]))\n",
    "\n",
    "        # Pad with False if one parent had more generated features\n",
    "        if len(child1_selection) < len(parent1_selection):\n",
    "             child1_selection = np.pad(child1_selection, (0, len(parent1_selection) - len(child1_selection)), mode='constant', constant_values=False)\n",
    "        if len(child2_selection) < len(parent2_selection):\n",
    "             child2_selection = np.pad(child2_selection, (0, len(parent2_selection) - len(child2_selection)), mode='constant', constant_values=False)\n",
    "        if len(child1_selection) < len(parent2_selection):\n",
    "             child1_selection = np.pad(child1_selection, (0, len(parent2_selection) - len(child1_selection)), mode='constant', constant_values=False)\n",
    "        if len(child2_selection) < len(parent1_selection):\n",
    "             child2_selection = np.pad(child2_selection, (0, len(parent1_selection) - len(child2_selection)), mode='constant', constant_values=False)\n",
    "\n",
    "\n",
    "        return child1_selection[:max(len(parent1_selection), len(parent2_selection))], child2_selection[:max(len(parent1_selection), len(parent2_selection))]\n",
    "\n",
    "\n",
    "    def _mutate(self, individual):\n",
    "        \"\"\"Muta un individuo (trees + selection).\"\"\"\n",
    "        mutated = {'transformation_trees': individual['transformation_trees'][:], 'selection': individual['selection'].copy()}\n",
    "\n",
    "        # Mutate transformation trees (with a certain probability)\n",
    "        if mutated['transformation_trees'] and random.random() < 0.7: # Probability to mutate trees\n",
    "             tree_to_mutate_idx = random.randint(0, len(mutated['transformation_trees']) - 1)\n",
    "             tree_to_mutate_root = mutated['transformation_trees'][tree_to_mutate_idx]\n",
    "\n",
    "             # Choose a random node in the tree\n",
    "             node_to_mutate = self._get_random_subtree(tree_to_mutate_root)\n",
    "\n",
    "             # Create a new random subtree\n",
    "             new_subtree = self._create_random_tree(self.max_tree_depth) # Consider depth limit for mutation?\n",
    "\n",
    "             # Replace the chosen node with the new subtree\n",
    "             mutated_tree_root = self._replace_subtree(tree_to_mutate_root, node_to_mutate, new_subtree)\n",
    "             mutated['transformation_trees'][tree_to_mutate_idx] = mutated_tree_root\n",
    "\n",
    "        # Mutate selection mask (with a certain probability)\n",
    "        if random.random() < 0.3: # Probability to mutate selection\n",
    "            n_flips = max(1, int(len(mutated['selection']) * 0.1))\n",
    "            if len(mutated['selection']) > 0:\n",
    "                flip_indices = random.sample(range(len(mutated['selection'])), min(n_flips, len(mutated['selection'])))\n",
    "                for idx in flip_indices:\n",
    "                    mutated['selection'][idx] = not mutated['selection'][idx]\n",
    "\n",
    "        # Ensure at least one feature is selected\n",
    "        if not np.any(mutated['selection']):\n",
    "            if len(mutated['selection']) > 0:\n",
    "                 mutated['selection'][random.randint(0, len(mutated['selection']) - 1)] = True\n",
    "            elif self.n_features_in_ > 0: # If no generated features, select an original feature\n",
    "                 mutated['selection'] = np.zeros(self.n_features_in_, dtype=bool)\n",
    "                 mutated['selection'][random.randint(0, self.n_features_in_ - 1)] = True\n",
    "            # If no features at all (original or generated), this state might indicate an issue\n",
    "\n",
    "\n",
    "        return mutated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d9ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'Comp_Ev/diabetes.csv' no encontrado. Usando el dataset de sklearn.datasets.load_diabetes()\n",
      "\n",
      "============================================================\n",
      "DATASET DIABETES\n",
      "============================================================\n",
      "Muestras: 442\n",
      "Features: 10\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BASELINE\n",
      "============================================================\n",
      "MAE en test: 41.9194\n",
      "\n",
      "============================================================\n",
      "INICIANDO BENCHMARK (GP)\n",
      "============================================================\n",
      "Total configuraciones a probar: 64\n",
      "Tiempo máximo por configuración: 15 minutos\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 1/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 3 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.2812\n",
      "Gen 2 - MEJORA! Fitness: 45.1782 (Tiempo: 0.0min)\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 2/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 5 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.3282\n",
      "Gen 1 - MEJORA! Fitness: 45.1798 (Tiempo: 0.1min)\n",
      "Gen 2 - MEJORA! Fitness: 45.1620 (Tiempo: 0.1min)\n",
      "Gen 3 - MEJORA! Fitness: 45.0484 (Tiempo: 0.1min)\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 3/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 3 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.6993\n",
      "Gen 1 - MEJORA! Fitness: 45.2643 (Tiempo: 0.1min)\n",
      "Gen 4 - MEJORA! Fitness: 45.1074 (Tiempo: 0.1min)\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 4/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 5 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.0960\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.6, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 5/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 3 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.1516\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 6/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 5 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 44.9704\n",
      "Gen 1 - MEJORA! Fitness: 44.9481 (Tiempo: 0.0min)\n",
      "Gen 5 - MEJORA! Fitness: 44.7993 (Tiempo: 0.1min)\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 7/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 3 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.5121\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 8/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 5 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.0738\n",
      "Error con la configuración {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.1, 'crossover_prob': 0.8, 'tournament_size': 5, 'validation_method': 'cv', 'cv_folds': 5, 'validation_size': None}: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "============================================================\n",
      "CONFIGURACIÓN 9/64\n",
      "============================================================\n",
      "Parámetros: {'maxtime': 900, 'population_size': 50, 'max_tree_depth': 3, 'mutation_prob': 0.2, 'crossover_prob': 0.6, 'tournament_size': 3, 'validation_method': 'cv', 'cv_folds': 3, 'validation_size': None}\n",
      "\n",
      "============================================================\n",
      "INICIANDO OPTIMIZACIÓN EVOLUTIVA (GP)\n",
      "============================================================\n",
      "Tiempo máximo: 900 segundos (15.0 minutos)\n",
      "Tamaño población: 50\n",
      "Features originales: 10\n",
      "Profundidad máxima del árbol: 3\n",
      "Método de validación: CV\n",
      "Cross-Validation con 3 folds\n",
      "============================================================\n",
      "\n",
      "Generación 0 - Fitness inicial: 45.4062\n",
      "Gen 1 - MEJORA! Fitness: 45.1325 (Tiempo: 0.0min)\n",
      "Gen 2 - MEJORA! Fitness: 44.9972 (Tiempo: 0.1min)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Use the new class name EvolutionaryOptimizerGP\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m EvolutionaryOptimizerGP(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 122\u001b[0m     X_train_opt \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     X_test_opt \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m    125\u001b[0m     optimized_model \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "Cell \u001b[0;32mIn[3], line 285\u001b[0m, in \u001b[0;36mEvolutionaryOptimizerGP.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fits the optimizer and then transforms the data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[3], line 193\u001b[0m, in \u001b[0;36mEvolutionaryOptimizerGP.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    190\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_fitness_holdout(ind, \u001b[38;5;241m*\u001b[39meval_data)\n\u001b[1;32m    191\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_fitness_cv(ind, \u001b[38;5;241m*\u001b[39meval_data)\n\u001b[1;32m    194\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Actualizar mejor individuo\u001b[39;00m\n\u001b[1;32m    197\u001b[0m current_best_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(fitness_scores)\n",
      "Cell \u001b[0;32mIn[3], line 193\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_fitness_holdout(ind, \u001b[38;5;241m*\u001b[39meval_data)\n\u001b[1;32m    191\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_fitness_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Actualizar mejor individuo\u001b[39;00m\n\u001b[1;32m    197\u001b[0m current_best_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(fitness_scores)\n",
      "Cell \u001b[0;32mIn[3], line 404\u001b[0m, in \u001b[0;36mEvolutionaryOptimizerGP._evaluate_fitness_cv\u001b[0;34m(self, individual, X, y)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Cross-validation\u001b[39;00m\n\u001b[1;32m    403\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression() \u001b[38;5;66;03m# Create a new model each time\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneg_mean_absolute_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use 1 job to avoid issues with multiprocessing and tree evaluation\u001b[39;49;00m\n\u001b[1;32m    409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mscores\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Penalización por complejidad (number of selected features)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# Adjust length of sample weights\u001b[39;00m\n\u001b[1;32m    865\u001b[0m fit_params \u001b[38;5;241m=\u001b[39m fit_params \u001b[38;5;28;01mif\u001b[39;00m fit_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 866\u001b[0m fit_params \u001b[38;5;241m=\u001b[39m \u001b[43m_check_method_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    867\u001b[0m score_params \u001b[38;5;241m=\u001b[39m score_params \u001b[38;5;28;01mif\u001b[39;00m score_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    868\u001b[0m score_params_train \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mscore_params, indices\u001b[38;5;241m=\u001b[39mtrain)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2173\u001b[0m, in \u001b[0;36m_check_method_params\u001b[0;34m(X, params, indices)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_method_params\u001b[39m(X, params, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check and validate the parameters passed to a specific\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;124;03m    method like `fit`.\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;124;03m        Validated parameters. We ensure that the values support indexing.\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2173\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing\n\u001b[1;32m   2175\u001b[0m     method_params_validated \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_key, param_value \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1053\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.setrecursionlimit(2000) # Increase recursion depth limit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time # Import the time module\n",
    "# Ejemplo de uso del EvolutionaryOptimizerGP\n",
    "import os\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "csv_path = 'Comp_Ev/diabetes.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X = df.drop('target', axis=1).values\n",
    "    y = df['target'].values\n",
    "else:\n",
    "    print(f\"Archivo '{csv_path}' no encontrado. Usando el dataset de sklearn.datasets.load_diabetes()\")\n",
    "    diabetes = load_diabetes(as_frame=True)\n",
    "    df = diabetes.frame\n",
    "    X = df.drop('target', axis=1).values\n",
    "    y = df['target'].values\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATASET DIABETES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Muestras: {X.shape[0]}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Baseline\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BASELINE\")\n",
    "print(f\"{'='*60}\")\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_error = mean_absolute_error(y_test, baseline_model.predict(X_test))\n",
    "print(f\"MAE en test: {baseline_error:.4f}\")\n",
    "\n",
    "# --- Benchmark Parameters ---\n",
    "# Define the parameter grid for the benchmark\n",
    "param_grid = {\n",
    "    'population_size': [50, 100],\n",
    "    'max_tree_depth': [3, 5],\n",
    "    'mutation_prob': [0.1, 0.2],\n",
    "    'crossover_prob': [0.6, 0.8],\n",
    "    'tournament_size': [3, 5],\n",
    "    'validation_method': ['cv'],\n",
    "    'cv_folds': [3, 5],\n",
    "    'validation_size': [0.2, 0.3]\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "# Calculate total configurations, ensuring to handle validation_method dependencies\n",
    "total_configs = 0\n",
    "for pop_size in param_grid['population_size']:\n",
    "    for max_depth in param_grid['max_tree_depth']:\n",
    "        for mut_prob in param_grid['mutation_prob']:\n",
    "            for cx_prob in param_grid['crossover_prob']:\n",
    "                for tourn_size in param_grid['tournament_size']:\n",
    "                    for val_method in param_grid['validation_method']:\n",
    "                        if val_method == 'cv':\n",
    "                            total_configs += len(param_grid['cv_folds'])\n",
    "                        else: # holdout\n",
    "                            total_configs += len(param_grid['validation_size'])\n",
    "\n",
    "config_count = 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"INICIANDO BENCHMARK (GP)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total configuraciones a probar: {total_configs}\")\n",
    "print(f\"Tiempo máximo por configuración: 15 minutos\") # Assuming 900 seconds maxtime\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "# Iterate through all parameter combinations\n",
    "for pop_size in param_grid['population_size']:\n",
    "    for max_depth in param_grid['max_tree_depth']:\n",
    "        for mut_prob in param_grid['mutation_prob']:\n",
    "            for cx_prob in param_grid['crossover_prob']:\n",
    "                for tourn_size in param_grid['tournament_size']:\n",
    "                    for val_method in param_grid['validation_method']:\n",
    "                        # Handle parameters specific to validation method\n",
    "                        if val_method == 'cv':\n",
    "                            cv_folds_list = param_grid['cv_folds']\n",
    "                            validation_size_list = [None] # Not used for CV\n",
    "                        else: # holdout\n",
    "                            cv_folds_list = [None] # Not used for holdout\n",
    "                            validation_size_list = param_grid['validation_size']\n",
    "\n",
    "                        for cv_folds in cv_folds_list:\n",
    "                            for val_size in validation_size_list:\n",
    "\n",
    "                                config_count += 1\n",
    "                                print(f\"\\n{'='*60}\")\n",
    "                                print(f\"CONFIGURACIÓN {config_count}/{total_configs}\")\n",
    "                                print(f\"{'='*60}\")\n",
    "\n",
    "                                params = {\n",
    "                                    'maxtime': 900, # 15 minutes\n",
    "                                    'population_size': pop_size,\n",
    "                                    'max_tree_depth': max_depth,\n",
    "                                    'mutation_prob': mut_prob,\n",
    "                                    'crossover_prob': cx_prob,\n",
    "                                    'tournament_size': tourn_size,\n",
    "                                    'validation_method': val_method,\n",
    "                                    'cv_folds': cv_folds,\n",
    "                                    'validation_size': val_size\n",
    "                                }\n",
    "                                print(\"Parámetros:\", params)\n",
    "\n",
    "                                try:\n",
    "                                    # Use the new class name EvolutionaryOptimizerGP\n",
    "                                    optimizer = EvolutionaryOptimizerGP(**params)\n",
    "                                    X_train_opt = optimizer.fit_transform(X_train, y_train)\n",
    "                                    X_test_opt = optimizer.transform(X_test)\n",
    "\n",
    "                                    optimized_model = LinearRegression()\n",
    "                                    optimized_model.fit(X_train_opt, y_train)\n",
    "                                    optimized_error = mean_absolute_error(y_test, optimized_model.predict(X_test_opt))\n",
    "\n",
    "                                    improvement = baseline_error - optimized_error\n",
    "\n",
    "                                    results.append({\n",
    "                                        'params': params,\n",
    "                                        'baseline_mae': baseline_error,\n",
    "                                        'optimized_mae': optimized_error,\n",
    "                                        'improvement': improvement,\n",
    "                                        'n_features_selected': np.sum(optimizer.best_selection_)\n",
    "                                    })\n",
    "                                    print(f\"Resultado MAE optimizado: {optimized_error:.4f}\")\n",
    "                                    print(f\"Mejora vs Baseline: {improvement:.4f}\")\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error con la configuración {params}: {e}\")\n",
    "                                    results.append({\n",
    "                                        'params': params,\n",
    "                                        'baseline_mae': baseline_error,\n",
    "                                        'optimized_mae': np.nan,\n",
    "                                        'improvement': np.nan,\n",
    "                                        'n_features_selected': np.nan\n",
    "                                    })\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BENCHMARK COMPLETADO (GP)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df.sort_values(by='optimized_mae', ascending=True))\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESUMEN\")\n",
    "    print(f\"{'='*60}\")\n",
    "    # Handle case where all runs failed (all optimized_mae are NaN)\n",
    "    if not results_df['optimized_mae'].isnull().all():\n",
    "        best_result = results_df.loc[results_df['optimized_mae'].idxmin()]\n",
    "        print(f\"Mejor MAE optimizado: {best_result['optimized_mae']:.4f}\")\n",
    "        print(f\"Mejor mejora vs Baseline: {best_result['improvement']:.4f}\")\n",
    "        print(f\"Parámetros de la mejor configuración:\")\n",
    "        for k, v in best_result['params'].items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    else:\n",
    "        print(\"No se pudieron obtener resultados válidos de la optimización para ninguna configuración.\")\n",
    "    print(f\"{'='*60}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
