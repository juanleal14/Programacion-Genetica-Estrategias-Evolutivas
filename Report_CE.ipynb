{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a899e893",
   "metadata": {},
   "source": [
    "# Report - Optimización de Conjuntos de Datos para ML\n",
    "A lo largo de este proyecto, hemos realizado diversas optimizaciones para conseguir las mejores transformaciones posibles respecto a la mayor variedad de modelos posibles.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Modificaciones Planteadas\n",
    "\n",
    "### 1.1 Síntesis de Atributos (Programación Genética)\n",
    "- **Objetivo**: Crear 4 nuevas features mediante árboles de expresiones matemáticas\n",
    "- **Operadores**: add, sub, mul, div, sqrt, square, log, sin, cos, tanh\n",
    "- **Terminales**: Variables originales X₀...Xₙ y constantes [-5, 5]\n",
    "\n",
    "### 1.2 Reducción de Dimensionalidad (Feature Selection)\n",
    "- **Objetivo**: Seleccionar subconjunto óptimo de features (originales + generadas)\n",
    "- **Método**: Algoritmo Genético con codificación binaria\n",
    "- **Rango**: Entre 3 y 15 features seleccionadas\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Técnicas de Computación Evolutiva\n",
    "\n",
    "### 2.1 Programación Genética (GP)\n",
    "**Población**: 100 individuos  \n",
    "**Representación**: Lista de árboles  \n",
    "**Profundidad**: 2-5 niveles  \n",
    "\n",
    "**Operadores Genéticos**:\n",
    "- Cruce a nivel de subárbol (prob. 0.8)\n",
    "- Mutación de nodos/valores (prob. 0.2)\n",
    "- Reemplazo aleatorio de árboles completos\n",
    "\n",
    "### 2.2 Algoritmo Genético (GA) para Feature Selection\n",
    "**Población**: 30 individuos  \n",
    "**Representación**: Vector binario [1,0,1,1,0...]  \n",
    "\n",
    "**Operadores Genéticos**:\n",
    "- Cruce uniforme (prob. 0.8)\n",
    "- Mutación por bit-flip 1-3 bits (prob. 0.3)\n",
    "- Selección por torneo tamaño 3\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Codificación, Operadores y Función de Fitness\n",
    "\n",
    "### 3.1 Codificación en GP\n",
    "\n",
    "- individuo = [árbol_1, árbol_2, árbol_3, árbol_4]\n",
    "- Ejemplo de árbol: (X₀ + sqrt(X₁))\n",
    "\n",
    "### 3.2 Función de Fitness (GP)\n",
    "\n",
    "fitness = MSE_cv + λ · complejidad\n",
    "\n",
    "Donde:\n",
    "- MSE_cv: Error cuadrático medio en 3-fold cross-validation\n",
    "- complejidad: Suma de nodos de todos los árboles\n",
    "- λ = 0.001, penalización por bloat (Excesiva expansión del árbol)\n",
    "- Modelo: Ridge Regression (α=1.0)\n",
    "\n",
    "### 3.3 Función de Fitness (Feature Selection)\n",
    "fitness = MSE_validation + 0.01 · n_features\n",
    "\n",
    "Donde:\n",
    "- MSE_validation: Error en conjunto de validación (20% train)\n",
    "- n_features: Número de features seleccionadas\n",
    "- Restricción: mínimo 2 features\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Resultados Obtenidos\n",
    "\n",
    "### 4.1 Configuración Experimental\n",
    "- **Datasets**: California Housing (20,640 × 8) y Diabetes (442 × 10)\n",
    "- **Tiempos**: 20 minutos y 1 hora\n",
    "- **Modelos evaluados**: 17 (Ridge, LinearReg, RF, XGBoost, SVR, etc.)\n",
    "- **Métricas**: MAE y MSE en test set (20%)\n",
    "\n",
    "### 4.2 Hallazgos Principales\n",
    "\n",
    "**Mejoras Consistentes**: \n",
    "- La mayoría de modelos mejoraron en ambos datasets\n",
    "- Mejoras más significativas en modelos lineales (+10-25% MSE)\n",
    "\n",
    "**Reducción Efectiva**: \n",
    "- De 8-10 features originales → 6-8 features optimizadas\n",
    "- Manteniendo o mejorando el rendimiento\n",
    "\n",
    "**Tiempo vs Rendimiento**:\n",
    "- 20 min: resultados buenos\n",
    "- 1 hora: mejoras incrementales ~2-5% adicionales\n",
    "\n",
    "**Generalización**:\n",
    "- Las transformaciones funcionan en múltiples tipos de modelos\n",
    "- No sobreajuste: mejoras en test, no solo en train\n",
    "\n",
    "### 4.3 Ejemplos de Features Generadas\n",
    "**California Housing:**\n",
    "\n",
    "- (Latitude * Longitude) / sqrt(MedInc)\n",
    "- log(AveRooms + AveBedrms)\n",
    "- sin(HouseAge) * Population\n",
    "\n",
    "**Diabetes:**\n",
    "\n",
    "- sqrt(bmi) * bp\n",
    "- (s1 + s2) / (s3 + ε)\n",
    "- square(age) - log(|sex|)\n",
    "\n",
    "*(Las tablas numéricas detalladas se encuentran en las celdas ejecutables del notebook)*\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusiones\n",
    "\n",
    "### 5.1 Estrategias Clave Implementadas\n",
    "- **Early stopping** con validación interna (100/30 gens)\n",
    "- **Protección robusta**: Clipping, NaN/Inf handling, división protegida\n",
    "- **Regularización**: Ridge + penalización por complejidad\n",
    "- **Evaluación robusta**: Cross-validation\n",
    "\n",
    "\n",
    "### 5.2 Trabajo Futuro\n",
    "1. **Paralelización**: Evaluación paralela de individuos con multiprocessing\n",
    "2. **Multi-objetivo**: Optimizar simultáneamente MSE, MAE y complejidad (Pareto)\n",
    "3. **Memoización**: Cachear evaluaciones de subárboles repetidos\n",
    "4. **Transfer learning**: Reutilizar features entre datasets similares\n",
    "\n",
    "### 5.3 Conclusión Final\n",
    "Las mejoras consistentes obtenidas en múltiples modelos y datasets validan la efectividad del enfoque híbrido **GP + GA**, posicionándolo como una solución viable para preprocesamiento automático en proyectos reales de Machine Learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Anexo de IA Generativa\n",
    "\n",
    "### 6.1 Uso de IA Generativa en el Proyecto\n",
    "\n",
    "Durante el desarrollo de este proyecto se utilizó **IA Generativa** (Claude, ChatGPT, Codex...) para:\n",
    "\n",
    "#### Tareas Realizadas:\n",
    "1. **Debugging de código**: Identificar errores en la evaluación de fitness\n",
    "2. **Optimización de rendimiento**: Sugerencias para reducir tiempo de ejecución\n",
    "3. **Exploración de hiperparámetros**: Recomendaciones de valores iniciales\n",
    "4. **Generación de visualizaciones**: Código para gráficos de evolución del fitness\n",
    "\n",
    "### 6.2 Ejemplos de Prompts Utilizados\n",
    "\n",
    "#### Prompt 1: Debugging\n",
    "```\n",
    "\"Tengo un error NaN en la evaluación de fitness de mi GP. \n",
    "El código es el siguiente: [...]. ¿Cómo puedo proteger contra \n",
    "divisiones por cero y valores infinitos?\"\n",
    "```\n",
    "\n",
    "**Resultado obtenido**:\n",
    "Solución implementada\n",
    "\n",
    "#### Prompt 2: Optimización\n",
    "```\n",
    "\"Mi algoritmo genético para feature selection es muy lento. \n",
    "Usa cross-validation 5-fold en cada evaluación. ¿Cómo puedo \n",
    "acelerar sin perder demasiada precisión?\"\n",
    "```\n",
    "\n",
    "**Resultado obtenido**:\n",
    "- Usar validación simple en lugar de CV durante la búsqueda\n",
    "- Aplicar CV solo al mejor individuo final\n",
    "- Reducir population_size de 50 a 30\n",
    "- Implementar early stopping agresivo (30 generaciones)\n",
    "\n",
    "#### Prompt 3: Estrategia de Reparto de Tiempo\n",
    "```\n",
    "\"Tengo maxtime=3600 segundos. ¿Cómo debería repartir el tiempo \n",
    "entre la fase de GP (síntesis) y GA (selección) para maximizar \n",
    "la mejora final?\"\n",
    "```\n",
    "\n",
    "**Resultado obtenido**:\n",
    "- 70% del tiempo para GP (síntesis de features)\n",
    "- 30% del tiempo para GA (selección)\n",
    "- Justificación: la síntesis es más compleja y beneficia más\n",
    "\n",
    "#### Prompt 4: Comparación de Modelos\n",
    "```\n",
    "\"Tengo resultados de 17 modelos con/sin optimización. \n",
    "Dame código para crear una tabla ordenada por mejora en MSE\"\n",
    "```\n",
    "\n",
    "**Resultado**: Código del bloque de análisis comparativo usado en el notebook\n",
    "\n",
    "#### Prompt 5: Visualización de Convergencia\n",
    "```\n",
    "\"Quiero graficar la evolución del mejor fitness a lo largo de \n",
    "las generaciones. ¿Cómo visualizo el progreso del GP?\"\n",
    "```\n",
    "\n",
    "**Resultado**: \n",
    "Código base de plot...\n",
    "\n",
    "## 7. Apéndices\n",
    "\n",
    "### Apéndice A: Uso Rápido\n",
    "```python\n",
    "from evopt import EvolutionaryOptimizer\n",
    "\n",
    "# Inicializar\n",
    "opt = EvolutionaryOptimizer(maxtime=3600)  # 1 hora\n",
    "\n",
    "# Entrenar\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Transformar\n",
    "X_train_optimized = opt.transform(X_train)\n",
    "X_test_optimized = opt.transform(X_test)\n",
    "\n",
    "# Entrenar modelo con datos optimizados\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train_optimized, y_train)\n",
    "predictions = model.predict(X_test_optimized)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Apéndice B: Tiempos de Ejecución Observados\n",
    "\n",
    "| Dataset | Config | GP Time | GA Time | Total | Generaciones GP | Generaciones GA |\n",
    "|---------|--------|---------|---------|-------|----------------|----------------|\n",
    "| California | 20min | 14.2 min | 5.8 min | 20.0 min | ~180 | ~45 |\n",
    "| California | 1h | 42.5 min | 17.5 min | 60.0 min | ~520 | ~95 |\n",
    "| Diabetes | 20min | 13.8 min | 6.2 min | 20.0 min | ~195 | ~50 |\n",
    "| Diabetes | 1h | 41.0 min | 19.0 min | 60.0 min | ~580 | ~105 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059aa874",
   "metadata": {},
   "source": [
    "# Código de Optimización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d38f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evopt import EvolutionaryOptimizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Quitar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importar las funciones y librerías adicionales\n",
    "from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import Lasso, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1f34d",
   "metadata": {},
   "source": [
    "## 1. -- 'california.csv' Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69110d",
   "metadata": {},
   "source": [
    "### 1.1 -- 20 Minutes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc7de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUACIÓN DEL SISTEMA\n",
      "======================================================================\n",
      "Dataset: 20640 instancias, 8 features\n",
      "Train: 16512 | Test: 4128\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BASELINE (Sin Optimización)\n",
      "======================================================================\n",
      "MAE: 0.5332\n",
      "MSE: 0.5558\n",
      "Features utilizadas: 8\n",
      "\n",
      "======================================================================\n",
      "OPTIMIZACIÓN EVOLUTIVA\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PROGRAMACIÓN GENÉTICA\n",
      "======================================================================\n",
      "Población: 100 | Features a crear: 4\n",
      "Profundidad máxima: 5\n",
      "Modelo de evaluación: RIDGE\n",
      "Tiempo asignado GP: 14.0min (840.0s)\n",
      "Tiempo asignado FS: 6.0min (360.0s)\n",
      "======================================================================\n",
      "\n",
      "Gen 1 - MEJORA! Val: 0.4855 | Train: 0.4457\n",
      "Gen 2 - MEJORA! Val: 0.4654 | Train: 0.4449\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('california.csv')\n",
    "\n",
    "\n",
    "# CASO CALIFORNIA HOUSING\n",
    "X = df.drop('MedHouseVal', axis=1).values\n",
    "y = df['MedHouseVal'].values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EVALUACIÓN DEL SISTEMA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dataset: {X.shape[0]} instancias, {X.shape[1]} features\")\n",
    "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# BASELINE: Modelo sin optimización\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimización)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "baseline = Ridge(alpha=1.0, random_state=42)\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_preds = baseline.predict(X_test)\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "\n",
    "print(f\"MAE: {baseline_mae:.4f}\")\n",
    "print(f\"MSE: {baseline_mse:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "\n",
    "# ========================================================================\n",
    "# OPTIMIZACIÓN CON PROGRAMACIÓN GENÉTICA + FEATURE SELECTION\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OPTIMIZACIÓN EVOLUTIVA\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Crear optimizador\n",
    "gp_optimizer = EvolutionaryOptimizer(\n",
    "maxtime=1200,  # 20 minutos (ajusta según necesites)\n",
    ")\n",
    "\n",
    "# Entrenar el optimizador (aprende transformaciones)\n",
    "gp_optimizer.fit(X_train, y_train)\n",
    "\n",
    "# Transformar los datos (aplicar las transformaciones aprendidas)\n",
    "X_train_optimized = gp_optimizer.transform(X_train)\n",
    "X_test_optimized = gp_optimizer.transform(X_test)\n",
    "\n",
    "if gp_optimizer.feature_selection_ is not None:\n",
    "    n_selected = np.sum(gp_optimizer.feature_selection_)\n",
    "    n_total = len(gp_optimizer.feature_selection_)\n",
    "    print(f\"Features seleccionadas: {n_selected}/{n_total}\")\n",
    "    \n",
    "    # Mostrar cuáles features se seleccionaron\n",
    "    print(f\"\\nFeatures seleccionadas:\")\n",
    "    selected_indices = np.where(gp_optimizer.feature_selection_)[0]\n",
    "    for idx in selected_indices:\n",
    "        if idx < X.shape[1]:\n",
    "            print(f\"  X{idx} (original)\")\n",
    "        else:\n",
    "            tree_idx = idx - X.shape[1]\n",
    "            if tree_idx < len(gp_optimizer.best_trees_):\n",
    "                print(f\"  {gp_optimizer.best_trees_[tree_idx].to_string()} (generada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2459be",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dffe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos adicionales para probar\n",
    "additional_models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'XGBoost': XGBRegressor(n_jobs=-1, random_state=42, verbosity=0),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'Bagging': BaggingRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'MLP': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'KernelRidge': KernelRidge(alpha=1.0)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROBANDO MODELOS ADICIONALES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in additional_models.items():\n",
    "    try:\n",
    "        print(f\"\\nProbando {name}...\")\n",
    "        \n",
    "        # Baseline\n",
    "        model_baseline = clone(model)\n",
    "        model_baseline.fit(X_train, y_train)\n",
    "        baseline_preds = model_baseline.predict(X_test)\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "        baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "        \n",
    "        # Con optimización\n",
    "        model_optimized = clone(model)\n",
    "        model_optimized.fit(X_train_optimized, y_train)\n",
    "        optimized_preds = model_optimized.predict(X_test_optimized)\n",
    "        \n",
    "        optimized_mae = mean_absolute_error(y_test, optimized_preds)\n",
    "        optimized_mse = mean_squared_error(y_test, optimized_preds)\n",
    "        \n",
    "        # Mejoras\n",
    "        mae_improvement = ((baseline_mae - optimized_mae) / baseline_mae * 100)\n",
    "        mse_improvement = ((baseline_mse - optimized_mse) / baseline_mse * 100)\n",
    "        \n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'MAE_Base': baseline_mae,\n",
    "            'MAE_Opt': optimized_mae,\n",
    "            'Mejora_MAE': mae_improvement,\n",
    "            'MSE_Base': baseline_mse,\n",
    "            'MSE_Opt': optimized_mse,\n",
    "            'Mejora_MSE': mse_improvement\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {baseline_mae:.4f} → {optimized_mae:.4f} ({mae_improvement:+.2f}%)\")\n",
    "        print(f\"  MSE: {baseline_mse:.4f} → {optimized_mse:.4f} ({mse_improvement:+.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"RESUMEN COMPLETO - TODOS LOS MODELOS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_sorted = df_results.sort_values('Mejora_MSE', ascending=False)\n",
    "\n",
    "print(f\"{'Modelo':<15} {'MAE Base':<10} {'MAE Opt':<10} {'Mejora MAE':<12} {'MSE Base':<10} {'MSE Opt':<10} {'Mejora MSE':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    print(f\"{row['Modelo']:<15} {row['MAE_Base']:<10.4f} {row['MAE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MAE']:+<12.2f}% {row['MSE_Base']:<10.4f} {row['MSE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MSE']:+<12.2f}%\")\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTADÍSTICAS GENERALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modelos que mejoraron MAE: {len(df_sorted[df_sorted['Mejora_MAE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Modelos que mejoraron MSE: {len(df_sorted[df_sorted['Mejora_MSE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Mejor mejora MAE: {df_sorted['Mejora_MAE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MAE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejor mejora MSE: {df_sorted['Mejora_MSE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MSE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejora promedio MAE: {df_sorted['Mejora_MAE'].mean():.2f}%\")\n",
    "print(f\"Mejora promedio MSE: {df_sorted['Mejora_MSE'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a65eb2",
   "metadata": {},
   "source": [
    "### 1.2 -- 1 Hour Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f192ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('california.csv')\n",
    "\n",
    "# CASO DIABETES\n",
    "#X = df.drop('target', axis=1).values\n",
    "#y = df['target'].values\n",
    "\n",
    "# CASO CALIFORNIA (descomentar si se usa otro dataset)\n",
    "X = df.drop('MedHouseVal', axis=1).values\n",
    "y = df['MedHouseVal'].values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EVALUACIÓN DEL SISTEMA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dataset: {X.shape[0]} instancias, {X.shape[1]} features\")\n",
    "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# BASELINE: Modelo sin optimización\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimización)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "baseline = Ridge(alpha=1.0, random_state=42)\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_preds = baseline.predict(X_test)\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "\n",
    "print(f\"MAE: {baseline_mae:.4f}\")\n",
    "print(f\"MSE: {baseline_mse:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "\n",
    "# ========================================================================\n",
    "# OPTIMIZACIÓN CON PROGRAMACIÓN GENÉTICA + FEATURE SELECTION\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OPTIMIZACIÓN EVOLUTIVA\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Crear optimizador\n",
    "gp_optimizer = EvolutionaryOptimizer(\n",
    "maxtime=3600,  # 20 minutos (ajusta según necesites)\n",
    ")\n",
    "\n",
    "# Entrenar el optimizador (aprende transformaciones)\n",
    "gp_optimizer.fit(X_train, y_train)\n",
    "\n",
    "# Transformar los datos (aplicar las transformaciones aprendidas)\n",
    "X_train_optimized = gp_optimizer.transform(X_train)\n",
    "X_test_optimized = gp_optimizer.transform(X_test)\n",
    "\n",
    "if gp_optimizer.feature_selection_ is not None:\n",
    "    n_selected = np.sum(gp_optimizer.feature_selection_)\n",
    "    n_total = len(gp_optimizer.feature_selection_)\n",
    "    print(f\"Features seleccionadas: {n_selected}/{n_total}\")\n",
    "    \n",
    "    # Mostrar cuáles features se seleccionaron\n",
    "    print(f\"\\nFeatures seleccionadas:\")\n",
    "    selected_indices = np.where(gp_optimizer.feature_selection_)[0]\n",
    "    for idx in selected_indices:\n",
    "        if idx < X.shape[1]:\n",
    "            print(f\"  X{idx} (original)\")\n",
    "        else:\n",
    "            tree_idx = idx - X.shape[1]\n",
    "            if tree_idx < len(gp_optimizer.best_trees_):\n",
    "                print(f\"  {gp_optimizer.best_trees_[tree_idx].to_string()} (generada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4bee6",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos adicionales para probar\n",
    "additional_models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'XGBoost': XGBRegressor(n_jobs=-1, random_state=42, verbosity=0),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'Bagging': BaggingRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'MLP': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'KernelRidge': KernelRidge(alpha=1.0)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROBANDO MODELOS ADICIONALES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in additional_models.items():\n",
    "    try:\n",
    "        print(f\"\\nProbando {name}...\")\n",
    "        \n",
    "        # Baseline\n",
    "        model_baseline = clone(model)\n",
    "        model_baseline.fit(X_train, y_train)\n",
    "        baseline_preds = model_baseline.predict(X_test)\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "        baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "        \n",
    "        # Con optimización\n",
    "        model_optimized = clone(model)\n",
    "        model_optimized.fit(X_train_optimized, y_train)\n",
    "        optimized_preds = model_optimized.predict(X_test_optimized)\n",
    "        \n",
    "        optimized_mae = mean_absolute_error(y_test, optimized_preds)\n",
    "        optimized_mse = mean_squared_error(y_test, optimized_preds)\n",
    "        \n",
    "        # Mejoras\n",
    "        mae_improvement = ((baseline_mae - optimized_mae) / baseline_mae * 100)\n",
    "        mse_improvement = ((baseline_mse - optimized_mse) / baseline_mse * 100)\n",
    "        \n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'MAE_Base': baseline_mae,\n",
    "            'MAE_Opt': optimized_mae,\n",
    "            'Mejora_MAE': mae_improvement,\n",
    "            'MSE_Base': baseline_mse,\n",
    "            'MSE_Opt': optimized_mse,\n",
    "            'Mejora_MSE': mse_improvement\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {baseline_mae:.4f} → {optimized_mae:.4f} ({mae_improvement:+.2f}%)\")\n",
    "        print(f\"  MSE: {baseline_mse:.4f} → {optimized_mse:.4f} ({mse_improvement:+.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"RESUMEN COMPLETO - TODOS LOS MODELOS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_sorted = df_results.sort_values('Mejora_MSE', ascending=False)\n",
    "\n",
    "print(f\"{'Modelo':<15} {'MAE Base':<10} {'MAE Opt':<10} {'Mejora MAE':<12} {'MSE Base':<10} {'MSE Opt':<10} {'Mejora MSE':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    print(f\"{row['Modelo']:<15} {row['MAE_Base']:<10.4f} {row['MAE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MAE']:+<12.2f}% {row['MSE_Base']:<10.4f} {row['MSE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MSE']:+<12.2f}%\")\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTADÍSTICAS GENERALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modelos que mejoraron MAE: {len(df_sorted[df_sorted['Mejora_MAE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Modelos que mejoraron MSE: {len(df_sorted[df_sorted['Mejora_MSE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Mejor mejora MAE: {df_sorted['Mejora_MAE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MAE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejor mejora MSE: {df_sorted['Mejora_MSE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MSE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejora promedio MAE: {df_sorted['Mejora_MAE'].mean():.2f}%\")\n",
    "print(f\"Mejora promedio MSE: {df_sorted['Mejora_MSE'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f9c9a",
   "metadata": {},
   "source": [
    "## 2. -- 'diabetes.csv' Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d33ddf",
   "metadata": {},
   "source": [
    "### 2.1 -- 20 Minutes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e11666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# CASO DIABETES\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "\n",
    "# CASO CALIFORNIA (descomentar si se usa otro dataset)\n",
    "#X = df.drop('MedHouseVal', axis=1).values\n",
    "#y = df['MedHouseVal'].values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EVALUACIÓN DEL SISTEMA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dataset: {X.shape[0]} instancias, {X.shape[1]} features\")\n",
    "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# BASELINE: Modelo sin optimización\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimización)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "baseline = Ridge(alpha=1.0, random_state=42)\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_preds = baseline.predict(X_test)\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "\n",
    "print(f\"MAE: {baseline_mae:.4f}\")\n",
    "print(f\"MSE: {baseline_mse:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "\n",
    "# ========================================================================\n",
    "# OPTIMIZACIÓN CON PROGRAMACIÓN GENÉTICA + FEATURE SELECTION\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OPTIMIZACIÓN EVOLUTIVA\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Crear optimizador\n",
    "gp_optimizer = EvolutionaryOptimizer(\n",
    "maxtime=1200,  # 20 minutos (ajusta según necesites)\n",
    ")\n",
    "\n",
    "# Entrenar el optimizador (aprende transformaciones)\n",
    "gp_optimizer.fit(X_train, y_train)\n",
    "\n",
    "# Transformar los datos (aplicar las transformaciones aprendidas)\n",
    "X_train_optimized = gp_optimizer.transform(X_train)\n",
    "X_test_optimized = gp_optimizer.transform(X_test)\n",
    "\n",
    "if gp_optimizer.feature_selection_ is not None:\n",
    "    n_selected = np.sum(gp_optimizer.feature_selection_)\n",
    "    n_total = len(gp_optimizer.feature_selection_)\n",
    "    print(f\"Features seleccionadas: {n_selected}/{n_total}\")\n",
    "    \n",
    "    # Mostrar cuáles features se seleccionaron\n",
    "    print(f\"\\nFeatures seleccionadas:\")\n",
    "    selected_indices = np.where(gp_optimizer.feature_selection_)[0]\n",
    "    for idx in selected_indices:\n",
    "        if idx < X.shape[1]:\n",
    "            print(f\"  X{idx} (original)\")\n",
    "        else:\n",
    "            tree_idx = idx - X.shape[1]\n",
    "            if tree_idx < len(gp_optimizer.best_trees_):\n",
    "                print(f\"  {gp_optimizer.best_trees_[tree_idx].to_string()} (generada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f15c3a",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36925af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos adicionales para probar\n",
    "additional_models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'XGBoost': XGBRegressor(n_jobs=-1, random_state=42, verbosity=0),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'Bagging': BaggingRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'MLP': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'KernelRidge': KernelRidge(alpha=1.0)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROBANDO MODELOS ADICIONALES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in additional_models.items():\n",
    "    try:\n",
    "        print(f\"\\nProbando {name}...\")\n",
    "        \n",
    "        # Baseline\n",
    "        model_baseline = clone(model)\n",
    "        model_baseline.fit(X_train, y_train)\n",
    "        baseline_preds = model_baseline.predict(X_test)\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "        baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "        \n",
    "        # Con optimización\n",
    "        model_optimized = clone(model)\n",
    "        model_optimized.fit(X_train_optimized, y_train)\n",
    "        optimized_preds = model_optimized.predict(X_test_optimized)\n",
    "        \n",
    "        optimized_mae = mean_absolute_error(y_test, optimized_preds)\n",
    "        optimized_mse = mean_squared_error(y_test, optimized_preds)\n",
    "        \n",
    "        # Mejoras\n",
    "        mae_improvement = ((baseline_mae - optimized_mae) / baseline_mae * 100)\n",
    "        mse_improvement = ((baseline_mse - optimized_mse) / baseline_mse * 100)\n",
    "        \n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'MAE_Base': baseline_mae,\n",
    "            'MAE_Opt': optimized_mae,\n",
    "            'Mejora_MAE': mae_improvement,\n",
    "            'MSE_Base': baseline_mse,\n",
    "            'MSE_Opt': optimized_mse,\n",
    "            'Mejora_MSE': mse_improvement\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {baseline_mae:.4f} → {optimized_mae:.4f} ({mae_improvement:+.2f}%)\")\n",
    "        print(f\"  MSE: {baseline_mse:.4f} → {optimized_mse:.4f} ({mse_improvement:+.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"RESUMEN COMPLETO - TODOS LOS MODELOS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_sorted = df_results.sort_values('Mejora_MSE', ascending=False)\n",
    "\n",
    "print(f\"{'Modelo':<15} {'MAE Base':<10} {'MAE Opt':<10} {'Mejora MAE':<12} {'MSE Base':<10} {'MSE Opt':<10} {'Mejora MSE':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    print(f\"{row['Modelo']:<15} {row['MAE_Base']:<10.4f} {row['MAE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MAE']:+<12.2f}% {row['MSE_Base']:<10.4f} {row['MSE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MSE']:+<12.2f}%\")\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTADÍSTICAS GENERALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modelos que mejoraron MAE: {len(df_sorted[df_sorted['Mejora_MAE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Modelos que mejoraron MSE: {len(df_sorted[df_sorted['Mejora_MSE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Mejor mejora MAE: {df_sorted['Mejora_MAE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MAE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejor mejora MSE: {df_sorted['Mejora_MSE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MSE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejora promedio MAE: {df_sorted['Mejora_MAE'].mean():.2f}%\")\n",
    "print(f\"Mejora promedio MSE: {df_sorted['Mejora_MSE'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e667fc1",
   "metadata": {},
   "source": [
    "### 2.2 -- 1 Hour Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# CASO DIABETES\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "\n",
    "# CASO CALIFORNIA (descomentar si se usa otro dataset)\n",
    "#X = df.drop('MedHouseVal', axis=1).values\n",
    "#y = df['MedHouseVal'].values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EVALUACIÓN DEL SISTEMA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dataset: {X.shape[0]} instancias, {X.shape[1]} features\")\n",
    "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# BASELINE: Modelo sin optimización\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimización)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "baseline = Ridge(alpha=1.0, random_state=42)\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_preds = baseline.predict(X_test)\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "\n",
    "print(f\"MAE: {baseline_mae:.4f}\")\n",
    "print(f\"MSE: {baseline_mse:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "\n",
    "# ========================================================================\n",
    "# OPTIMIZACIÓN CON PROGRAMACIÓN GENÉTICA + FEATURE SELECTION\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OPTIMIZACIÓN EVOLUTIVA\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Crear optimizador\n",
    "gp_optimizer = EvolutionaryOptimizer(\n",
    "maxtime=3600,  # 20 minutos (ajusta según necesites)\n",
    ")\n",
    "\n",
    "# Entrenar el optimizador (aprende transformaciones)\n",
    "gp_optimizer.fit(X_train, y_train)\n",
    "\n",
    "# Transformar los datos (aplicar las transformaciones aprendidas)\n",
    "X_train_optimized = gp_optimizer.transform(X_train)\n",
    "X_test_optimized = gp_optimizer.transform(X_test)\n",
    "\n",
    "if gp_optimizer.feature_selection_ is not None:\n",
    "    n_selected = np.sum(gp_optimizer.feature_selection_)\n",
    "    n_total = len(gp_optimizer.feature_selection_)\n",
    "    print(f\"Features seleccionadas: {n_selected}/{n_total}\")\n",
    "    \n",
    "    # Mostrar cuáles features se seleccionaron\n",
    "    print(f\"\\nFeatures seleccionadas:\")\n",
    "    selected_indices = np.where(gp_optimizer.feature_selection_)[0]\n",
    "    for idx in selected_indices:\n",
    "        if idx < X.shape[1]:\n",
    "            print(f\"  X{idx} (original)\")\n",
    "        else:\n",
    "            tree_idx = idx - X.shape[1]\n",
    "            if tree_idx < len(gp_optimizer.best_trees_):\n",
    "                print(f\"  {gp_optimizer.best_trees_[tree_idx].to_string()} (generada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397b82b",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5731bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos adicionales para probar\n",
    "additional_models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'XGBoost': XGBRegressor(n_jobs=-1, random_state=42, verbosity=0),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'Bagging': BaggingRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'MLP': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'KernelRidge': KernelRidge(alpha=1.0)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROBANDO MODELOS ADICIONALES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in additional_models.items():\n",
    "    try:\n",
    "        print(f\"\\nProbando {name}...\")\n",
    "        \n",
    "        # Baseline\n",
    "        model_baseline = clone(model)\n",
    "        model_baseline.fit(X_train, y_train)\n",
    "        baseline_preds = model_baseline.predict(X_test)\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "        baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "        \n",
    "        # Con optimización\n",
    "        model_optimized = clone(model)\n",
    "        model_optimized.fit(X_train_optimized, y_train)\n",
    "        optimized_preds = model_optimized.predict(X_test_optimized)\n",
    "        \n",
    "        optimized_mae = mean_absolute_error(y_test, optimized_preds)\n",
    "        optimized_mse = mean_squared_error(y_test, optimized_preds)\n",
    "        \n",
    "        # Mejoras\n",
    "        mae_improvement = ((baseline_mae - optimized_mae) / baseline_mae * 100)\n",
    "        mse_improvement = ((baseline_mse - optimized_mse) / baseline_mse * 100)\n",
    "        \n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'MAE_Base': baseline_mae,\n",
    "            'MAE_Opt': optimized_mae,\n",
    "            'Mejora_MAE': mae_improvement,\n",
    "            'MSE_Base': baseline_mse,\n",
    "            'MSE_Opt': optimized_mse,\n",
    "            'Mejora_MSE': mse_improvement\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {baseline_mae:.4f} → {optimized_mae:.4f} ({mae_improvement:+.2f}%)\")\n",
    "        print(f\"  MSE: {baseline_mse:.4f} → {optimized_mse:.4f} ({mse_improvement:+.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"RESUMEN COMPLETO - TODOS LOS MODELOS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_sorted = df_results.sort_values('Mejora_MSE', ascending=False)\n",
    "\n",
    "print(f\"{'Modelo':<15} {'MAE Base':<10} {'MAE Opt':<10} {'Mejora MAE':<12} {'MSE Base':<10} {'MSE Opt':<10} {'Mejora MSE':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    print(f\"{row['Modelo']:<15} {row['MAE_Base']:<10.4f} {row['MAE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MAE']:+<12.2f}% {row['MSE_Base']:<10.4f} {row['MSE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MSE']:+<12.2f}%\")\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTADÍSTICAS GENERALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modelos que mejoraron MAE: {len(df_sorted[df_sorted['Mejora_MAE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Modelos que mejoraron MSE: {len(df_sorted[df_sorted['Mejora_MSE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Mejor mejora MAE: {df_sorted['Mejora_MAE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MAE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejor mejora MSE: {df_sorted['Mejora_MSE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MSE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejora promedio MAE: {df_sorted['Mejora_MAE'].mean():.2f}%\")\n",
    "print(f\"Mejora promedio MSE: {df_sorted['Mejora_MSE'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36c06c",
   "metadata": {},
   "source": [
    "## 3. -- 'diabetes_smote.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8431b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('diabetes_smote.csv')\n",
    "\n",
    "# CASO DIABETES\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "\n",
    "# CASO CALIFORNIA (descomentar si se usa otro dataset)\n",
    "#X = df.drop('MedHouseVal', axis=1).values\n",
    "#y = df['MedHouseVal'].values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EVALUACIÓN DEL SISTEMA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dataset: {X.shape[0]} instancias, {X.shape[1]} features\")\n",
    "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# BASELINE: Modelo sin optimización\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimización)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "baseline = Ridge(alpha=1.0, random_state=42)\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_preds = baseline.predict(X_test)\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "\n",
    "print(f\"MAE: {baseline_mae:.4f}\")\n",
    "print(f\"MSE: {baseline_mse:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "\n",
    "# ========================================================================\n",
    "# OPTIMIZACIÓN CON PROGRAMACIÓN GENÉTICA + FEATURE SELECTION\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OPTIMIZACIÓN EVOLUTIVA\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Crear optimizador\n",
    "gp_optimizer = EvolutionaryOptimizer(\n",
    "maxtime=3600,  # 20 minutos (ajusta según necesites)\n",
    ")\n",
    "\n",
    "# Entrenar el optimizador (aprende transformaciones)\n",
    "gp_optimizer.fit(X_train, y_train)\n",
    "\n",
    "# Transformar los datos (aplicar las transformaciones aprendidas)\n",
    "X_train_optimized = gp_optimizer.transform(X_train)\n",
    "X_test_optimized = gp_optimizer.transform(X_test)\n",
    "\n",
    "if gp_optimizer.feature_selection_ is not None:\n",
    "    n_selected = np.sum(gp_optimizer.feature_selection_)\n",
    "    n_total = len(gp_optimizer.feature_selection_)\n",
    "    print(f\"Features seleccionadas: {n_selected}/{n_total}\")\n",
    "    \n",
    "    # Mostrar cuáles features se seleccionaron\n",
    "    print(f\"\\nFeatures seleccionadas:\")\n",
    "    selected_indices = np.where(gp_optimizer.feature_selection_)[0]\n",
    "    for idx in selected_indices:\n",
    "        if idx < X.shape[1]:\n",
    "            print(f\"  X{idx} (original)\")\n",
    "        else:\n",
    "            tree_idx = idx - X.shape[1]\n",
    "            if tree_idx < len(gp_optimizer.best_trees_):\n",
    "                print(f\"  {gp_optimizer.best_trees_[tree_idx].to_string()} (generada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eef457",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos adicionales para probar\n",
    "additional_models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'XGBoost': XGBRegressor(n_jobs=-1, random_state=42, verbosity=0),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'Bagging': BaggingRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'MLP': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'KernelRidge': KernelRidge(alpha=1.0)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROBANDO MODELOS ADICIONALES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in additional_models.items():\n",
    "    try:\n",
    "        print(f\"\\nProbando {name}...\")\n",
    "        \n",
    "        # Baseline\n",
    "        model_baseline = clone(model)\n",
    "        model_baseline.fit(X_train, y_train)\n",
    "        baseline_preds = model_baseline.predict(X_test)\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "        baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "        \n",
    "        # Con optimización\n",
    "        model_optimized = clone(model)\n",
    "        model_optimized.fit(X_train_optimized, y_train)\n",
    "        optimized_preds = model_optimized.predict(X_test_optimized)\n",
    "        \n",
    "        optimized_mae = mean_absolute_error(y_test, optimized_preds)\n",
    "        optimized_mse = mean_squared_error(y_test, optimized_preds)\n",
    "        \n",
    "        # Mejoras\n",
    "        mae_improvement = ((baseline_mae - optimized_mae) / baseline_mae * 100)\n",
    "        mse_improvement = ((baseline_mse - optimized_mse) / baseline_mse * 100)\n",
    "        \n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'MAE_Base': baseline_mae,\n",
    "            'MAE_Opt': optimized_mae,\n",
    "            'Mejora_MAE': mae_improvement,\n",
    "            'MSE_Base': baseline_mse,\n",
    "            'MSE_Opt': optimized_mse,\n",
    "            'Mejora_MSE': mse_improvement\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {baseline_mae:.4f} → {optimized_mae:.4f} ({mae_improvement:+.2f}%)\")\n",
    "        print(f\"  MSE: {baseline_mse:.4f} → {optimized_mse:.4f} ({mse_improvement:+.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"RESUMEN COMPLETO - TODOS LOS MODELOS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_sorted = df_results.sort_values('Mejora_MSE', ascending=False)\n",
    "\n",
    "print(f\"{'Modelo':<15} {'MAE Base':<10} {'MAE Opt':<10} {'Mejora MAE':<12} {'MSE Base':<10} {'MSE Opt':<10} {'Mejora MSE':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    print(f\"{row['Modelo']:<15} {row['MAE_Base']:<10.4f} {row['MAE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MAE']:+<12.2f}% {row['MSE_Base']:<10.4f} {row['MSE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MSE']:+<12.2f}%\")\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTADÍSTICAS GENERALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modelos que mejoraron MAE: {len(df_sorted[df_sorted['Mejora_MAE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Modelos que mejoraron MSE: {len(df_sorted[df_sorted['Mejora_MSE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Mejor mejora MAE: {df_sorted['Mejora_MAE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MAE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejor mejora MSE: {df_sorted['Mejora_MSE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MSE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejora promedio MAE: {df_sorted['Mejora_MAE'].mean():.2f}%\")\n",
    "print(f\"Mejora promedio MSE: {df_sorted['Mejora_MSE'].mean():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
