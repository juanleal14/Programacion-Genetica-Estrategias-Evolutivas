import numpy as np
import pandas as pd
import time
import random
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_validate
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import RobustScaler
import warnings
warnings.filterwarnings('ignore')


class GPNode:
    """Nodo de 谩rbol de programaci贸n gen茅tica."""
    
    def __init__(self, value, children=None, node_type='terminal'):
        self.value = value
        self.children = children or []
        self.node_type = node_type  # 'terminal' o 'function'
    
    def evaluate(self, X):
        """Eval煤a el nodo con los datos X."""
        if self.node_type == 'terminal':
            if isinstance(self.value, int):  # ndice de feature
                return X[:, self.value]
            else:  # Constante
                return np.full(X.shape[0], self.value)
        
        else:  # function
            if self.value == 'add':
                return self.children[0].evaluate(X) + self.children[1].evaluate(X)
            elif self.value == 'sub':
                return self.children[0].evaluate(X) - self.children[1].evaluate(X)
            elif self.value == 'mul':
                return self.children[0].evaluate(X) * self.children[1].evaluate(X)
            elif self.value == 'div':
                right = self.children[1].evaluate(X)
                return self.children[0].evaluate(X) / (right + 1e-6)
            elif self.value == 'sqrt':
                return np.sqrt(np.abs(self.children[0].evaluate(X)) + 1e-6)
            elif self.value == 'square':
                val = self.children[0].evaluate(X)
                return np.clip(val, -100, 100) ** 2
            elif self.value == 'log':
                return np.log(np.abs(self.children[0].evaluate(X)) + 1)
            elif self.value == 'abs':
                return np.abs(self.children[0].evaluate(X))
            elif self.value == 'sin':
                return np.sin(self.children[0].evaluate(X))
            elif self.value == 'cos':
                return np.cos(self.children[0].evaluate(X))
            elif self.value == 'tanh':
                return np.tanh(self.children[0].evaluate(X))
    
    def copy(self):
        """Copia profunda del nodo."""
        return GPNode(
            self.value,
            [child.copy() for child in self.children],
            self.node_type
        )
    
    def size(self):
        """Tama帽o del 谩rbol (n煤mero de nodos)."""
        return 1 + sum(child.size() for child in self.children)
    
    def depth(self):
        """Profundidad del 谩rbol."""
        if not self.children:
            return 1
        return 1 + max(child.depth() for child in self.children)
    
    def to_string(self):
        """Representaci贸n en string del 谩rbol."""
        if self.node_type == 'terminal':
            if isinstance(self.value, int):
                return f"X{self.value}"
            else:
                return f"{self.value:.3f}"
        else:
            if len(self.children) == 1:
                return f"{self.value}({self.children[0].to_string()})"
            else:
                return f"({self.children[0].to_string()} {self.value} {self.children[1].to_string()})"


class EvolutionaryOptimizer(BaseEstimator, TransformerMixin):
    """Optimizador de programaci贸n gen茅tica para crear features."""
    
    def __init__(self, maxtime=1200, population_size=50, n_features_to_create=8,
                 mutation_prob=0.2, crossover_prob=0.8, tournament_size=5,
                 max_depth=5, elite_size=0.1):
        self.maxtime = maxtime
        self.population_size = population_size
        self.n_features_to_create = n_features_to_create
        self.mutation_prob = mutation_prob
        self.crossover_prob = crossover_prob
        self.tournament_size = tournament_size
        self.max_depth = max_depth
        self.elite_size = int(population_size * elite_size)
        
        # Funciones disponibles
        self.functions = {
            'add': 2, 'sub': 2, 'mul': 2, 'div': 2,  # Binarias
            'sqrt': 1, 'square': 1, 'log': 1, 'abs': 1, 'sin': 1, 'cos': 1, 'tanh': 1  # Unarias
        }
        
        self.best_trees_ = []
        self.best_fitness_ = float('inf')
        self.fitness_history_ = []
        self.best_metrics_ = {'mae': None, 'mse': None}
    def fit(self, X, y):
        """Entrena usando programaci贸n gen茅tica."""
        start_time = time.time()
        
        if hasattr(X, 'values'):
            X = X.values
        if hasattr(y, 'values'):
            y = y.values
        
        self.scaler_ = RobustScaler()
        X_scaled = self.scaler_.fit_transform(X)
        self.n_features_in_ = X.shape[1]
        
        print(f"\n{'='*70}")
        print(f"PROGRAMACIN GENTICA")
        print(f"{'='*70}")
        print(f"Poblaci贸n: {self.population_size} | Features a crear: {self.n_features_to_create}")
        print(f"Profundidad m谩xima: {self.max_depth} | Tiempo: {self.maxtime/60:.1f}min")
        print(f"{'='*70}\n")
        
        # Inicializar poblaci贸n de 谩rboles
        population = [self._create_random_tree() for _ in range(self.population_size)]
        
        generation = 0
        stagnation = 0
        
        while (time.time() - start_time) < self.maxtime:
            generation += 1
            
            # Evaluar poblaci贸n
            fitness_scores = []
            metric_scores = []
            for individual in population:
                fitness, metrics = self._evaluate_individual(individual, X_scaled, y)
                fitness_scores.append(fitness)
                metric_scores.append(metrics)

            # Actualizar mejor
            best_idx = np.argmin(fitness_scores)
            if fitness_scores[best_idx] < self.best_fitness_:
                self.best_fitness_ = fitness_scores[best_idx]
                self.best_trees_ = [tree.copy() for tree in population[best_idx]]
                self.best_metrics_ = metric_scores[best_idx]
                stagnation = 0
                print(f"Gen {generation} - MEJORA! Fitness (MSE+penalizaci贸n): {self.best_fitness_:.4f}")
                if (
                    self.best_metrics_['mse'] is not None
                    and self.best_metrics_['mae'] is not None
                    and np.isfinite(self.best_metrics_['mse'])
                    and np.isfinite(self.best_metrics_['mae'])
                ):
                    print(f"  MSE: {self.best_metrics_['mse']:.4f} | MAE: {self.best_metrics_['mae']:.4f}")
                if self.best_trees_:
                    print(f"  Mejor 谩rbol: {self.best_trees_[0].to_string()}")
            else:
                stagnation += 1
            
            # CAMBIO DE RAMA cada 200 generaciones de estancamiento
            if stagnation > 0 and stagnation % 200 == 0:
                print(f"\n CAMBIO DE RAMA en generaci贸n {generation}")
                print(f"   Estancamiento: {stagnation} generaciones")
                print(f"   Diversificando poblaci贸n...")
                
                # Mantener solo el 20% de elite
                elite_size_restart = max(1, self.population_size // 5)
                elite_indices = np.argsort(fitness_scores)[:elite_size_restart]
                
                # Crear nueva poblaci贸n diversa
                new_population_restart = []
                
                # Mantener elite
                for idx in elite_indices:
                    new_population_restart.append([tree.copy() for tree in population[idx]])
                
                # Generar resto completamente nuevo con m谩s diversidad
                while len(new_population_restart) < self.population_size:
                    # Crear individuos m谩s diversos
                    new_individual = []
                    for _ in range(self.n_features_to_create):
                        # rboles m谩s profundos y diversos
                        depth = random.randint(3, self.max_depth + 1)
                        tree = self._generate_tree(max_depth=depth)
                        new_individual.append(tree)
                    new_population_restart.append(new_individual)
                
                population = new_population_restart
                print(f"   Nueva poblaci贸n creada con {len(population)} individuos")
            
            self.fitness_history_.append(self.best_fitness_)
            
            # Nueva generaci贸n
            new_population = []
            
            # Elitismo
            elite_indices = np.argsort(fitness_scores)[:self.elite_size]
            for idx in elite_indices:
                new_population.append([tree.copy() for tree in population[idx]])
            
            # Generar resto
            while len(new_population) < self.population_size:
                parent1 = self._tournament_selection(population, fitness_scores)
                parent2 = self._tournament_selection(population, fitness_scores)
                
                if random.random() < self.crossover_prob:
                    child1, child2 = self._crossover_trees(parent1, parent2)
                else:
                    child1 = [tree.copy() for tree in parent1]
                    child2 = [tree.copy() for tree in parent2]
                
                if random.random() < self.mutation_prob:
                    child1 = self._mutate_trees(child1)
                if random.random() < self.mutation_prob:
                    child2 = self._mutate_trees(child2)
                
                new_population.extend([child1, child2])
            
            population = new_population[:self.population_size]
            
            if generation % 50 == 0:
                elapsed = time.time() - start_time
                metrics_msg = ""
                if (
                    self.best_metrics_['mse'] is not None
                    and self.best_metrics_['mae'] is not None
                    and np.isfinite(self.best_metrics_['mse'])
                    and np.isfinite(self.best_metrics_['mae'])
                ):
                    metrics_msg = (f" | Mejor MSE: {self.best_metrics_['mse']:.4f}"
                                   f" | Mejor MAE: {self.best_metrics_['mae']:.4f}")
                print(f"Gen {generation} | Fitness: {self.best_fitness_:.4f}{metrics_msg} | " +
                      f"Tiempo: {elapsed/60:.1f}min | Estancamiento: {stagnation}")
        
        print(f"\nCompletado en {generation} generaciones")
        if (
            self.best_metrics_['mse'] is not None
            and self.best_metrics_['mae'] is not None
            and np.isfinite(self.best_metrics_['mse'])
            and np.isfinite(self.best_metrics_['mae'])
        ):
            print(f"Mejor MSE: {self.best_metrics_['mse']:.4f} | Mejor MAE: {self.best_metrics_['mae']:.4f}")
        print(f"Mejores 谩rboles encontrados:")
        for i, tree in enumerate(self.best_trees_):
            print(f"  {i+1}: {tree.to_string()}")

        return self
    
    def transform(self, X):
        """Transforma datos usando los mejores 谩rboles."""
        if not self.best_trees_:
            raise ValueError("No entrenado")
        
        if hasattr(X, 'values'):
            X = X.values
        
        X_scaled = self.scaler_.transform(X)
        X_new = X_scaled.copy()
        
        # Aplicar cada 谩rbol como nueva feature
        for tree in self.best_trees_:
            try:
                new_feature = tree.evaluate(X_scaled)
                # Limpieza num茅rica
                new_feature = np.nan_to_num(new_feature, nan=0.0, posinf=100.0, neginf=-100.0)
                new_feature = np.clip(new_feature, -1000, 1000)
                X_new = np.column_stack([X_new, new_feature])
            except:
                X_new = np.column_stack([X_new, np.zeros(X_scaled.shape[0])])
        
        return X_new
    
    def predict(self, X):
        """Predice usando regresi贸n lineal sobre features transformadas."""
        X_transformed = self.transform(X)
        return self.model_.predict(X_transformed)
    
    def _create_random_tree(self):
        """Crea conjunto aleatorio de 谩rboles."""
        trees = []
        for _ in range(self.n_features_to_create):
            tree = self._generate_tree(max_depth=random.randint(2, self.max_depth))
            trees.append(tree)
        return trees
    
    def _generate_tree(self, max_depth):
        """Genera un 谩rbol aleatorio."""
        if max_depth <= 1 or random.random() < 0.3:  # Terminal
            if random.random() < 0.8:  # Feature
                return GPNode(random.randint(0, self.n_features_in_ - 1), node_type='terminal')
            else:  # Constante
                return GPNode(random.uniform(-5, 5), node_type='terminal')
        
        else:  # Funci贸n
            func_name = random.choice(list(self.functions.keys()))
            arity = self.functions[func_name]
            children = [self._generate_tree(max_depth - 1) for _ in range(arity)]
            return GPNode(func_name, children, node_type='function')
    
    def _evaluate_individual(self, trees, X, y):
        """Eval煤a un individuo (conjunto de 谩rboles)."""
        try:
            # Crear features usando los 谩rboles
            X_new = X.copy()
            for tree in trees:
                try:
                    new_feature = tree.evaluate(X)
                    new_feature = np.nan_to_num(new_feature, nan=0.0, posinf=100.0, neginf=-100.0)
                    new_feature = np.clip(new_feature, -1000, 1000)
                    X_new = np.column_stack([X_new, new_feature])
                except:
                    X_new = np.column_stack([X_new, np.zeros(X.shape[0])])

            # Verificar estabilidad
            if np.any(np.abs(X_new) > 1e6) or np.any(np.std(X_new, axis=0) < 1e-10):
                return 1e6, {'mae': float('inf'), 'mse': float('inf')}

            # Cross-validation con MAE y MSE
            model = LinearRegression()
            cv_results = cross_validate(
                model,
                X_new,
                y,
                cv=3,
                scoring={'mae': 'neg_mean_absolute_error', 'mse': 'neg_mean_squared_error'},
                n_jobs=-1
            )

            if (np.any(np.isnan(cv_results['test_mae'])) or
                    np.any(np.isnan(cv_results['test_mse']))):
                return 1e6, {'mae': float('inf'), 'mse': float('inf')}

            mae = -cv_results['test_mae'].mean()
            mse = -cv_results['test_mse'].mean()

            # Penalizaci贸n por complejidad
            complexity = sum(tree.size() for tree in trees)
            penalty = 0.001 * complexity

            fitness = mse + penalty

            # Guardar modelo si es el mejor hasta ahora
            if fitness < self.best_fitness_:
                model.fit(X_new, y)
                self.model_ = model

            return fitness, {'mae': mae, 'mse': mse}

        except:
            return 1e6, {'mae': float('inf'), 'mse': float('inf')}
    
    def _tournament_selection(self, population, fitness_scores):
        """Selecci贸n por torneo."""
        tournament_idx = random.sample(range(len(population)), 
                                     min(self.tournament_size, len(population)))
        tournament_fitness = [fitness_scores[i] for i in tournament_idx]
        winner_idx = tournament_idx[np.argmin(tournament_fitness)]
        return [tree.copy() for tree in population[winner_idx]]
    
    def _crossover_trees(self, parent1, parent2):
        """Cruce entre conjuntos de 谩rboles."""
        child1 = [tree.copy() for tree in parent1]
        child2 = [tree.copy() for tree in parent2]
        
        # Intercambiar algunos 谩rboles
        n_swap = random.randint(1, min(len(child1), len(child2)) // 2)
        indices = random.sample(range(min(len(child1), len(child2))), n_swap)
        
        for idx in indices:
            child1[idx], child2[idx] = child2[idx].copy(), child1[idx].copy()
        
        # Cruce dentro de 谩rboles individuales
        for i in range(min(len(child1), len(child2))):
            if random.random() < 0.3:
                child1[i], child2[i] = self._crossover_single_tree(child1[i], child2[i])
        
        return child1, child2
    
    def _crossover_single_tree(self, tree1, tree2):
        """Cruce entre dos 谩rboles individuales."""
        # Seleccionar nodos aleatorios para intercambiar
        if tree1.children and tree2.children and random.random() < 0.7:
            # Intercambiar sub谩rboles
            idx1 = random.randint(0, len(tree1.children) - 1)
            idx2 = random.randint(0, len(tree2.children) - 1)
            tree1.children[idx1], tree2.children[idx2] = tree2.children[idx2].copy(), tree1.children[idx1].copy()
        
        return tree1, tree2
    
    def _mutate_trees(self, trees):
        """Mutaci贸n de conjunto de 谩rboles."""
        mutated = [tree.copy() for tree in trees]
        
        for i, tree in enumerate(mutated):
            if random.random() < 0.3:  # Mutar 谩rbol
                mutated[i] = self._mutate_single_tree(tree)
        
        # Ocasionalmente reemplazar 谩rbol completo
        if random.random() < 0.1:
            idx = random.randint(0, len(mutated) - 1)
            mutated[idx] = self._generate_tree(max_depth=random.randint(2, self.max_depth))
        
        return mutated
    
    def _mutate_single_tree(self, tree):
        """Mutaci贸n de un 谩rbol individual."""
        mutated = tree.copy()
        
        if random.random() < 0.5 and mutated.children:  # Mutar funci贸n
            mutated.value = random.choice(list(self.functions.keys()))
            # Ajustar hijos si es necesario
            required_arity = self.functions[mutated.value]
            while len(mutated.children) < required_arity:
                mutated.children.append(self._generate_tree(max_depth=2))
            mutated.children = mutated.children[:required_arity]
        
        elif mutated.node_type == 'terminal':  # Mutar terminal
            if isinstance(mutated.value, int):  # Feature
                mutated.value = random.randint(0, self.n_features_in_ - 1)
            else:  # Constante
                mutated.value = random.uniform(-5, 5)
        
        # Mutar recursivamente hijos
        for child in mutated.children:
            if random.random() < 0.2:
                child = self._mutate_single_tree(child)
        
        return mutated


def evolutionary_feature_selection(X_train, y_train, X_test, y_test, 
                                 population_size=30, generations=100):
    """Selecci贸n evolutiva de features."""
    n_features = X_train.shape[1]
    
    # Inicializar poblaci贸n (vectores binarios)
    population = []
    for _ in range(population_size):
        # Asegurar al menos 3 features seleccionadas
        individual = np.zeros(n_features, dtype=bool)
        n_selected = random.randint(3, min(15, n_features))
        selected_idx = random.sample(range(n_features), n_selected)
        individual[selected_idx] = True
        population.append(individual)
    
    best_fitness = float('inf')
    best_individual = None
    best_metrics = {'mae': None, 'mse': None}
    stagnation = 0

    for gen in range(generations):
        # Evaluar poblaci贸n
        fitness_scores = []
        metric_scores = []
        for individual in population:
            fitness, metrics = evaluate_feature_subset(individual, X_train, y_train, X_test, y_test)
            fitness_scores.append(fitness)
            metric_scores.append(metrics)

        # Actualizar mejor
        current_best_idx = np.argmin(fitness_scores)
        if fitness_scores[current_best_idx] < best_fitness:
            best_fitness = fitness_scores[current_best_idx]
            best_individual = population[current_best_idx].copy()
            best_metrics = metric_scores[current_best_idx]
            stagnation = 0
            if gen % 20 == 0 or gen < 10:
                n_selected = np.sum(best_individual)
                print(f"  Gen {gen}: MSE = {best_metrics['mse']:.4f} | MAE = {best_metrics['mae']:.4f} | "
                      f"Features: {n_selected}")
        else:
            stagnation += 1
        
        # Early stopping para feature selection
        if stagnation >= 30:
            break
        
        # Nueva generaci贸n
        new_population = []
        
        # Elitismo (10%)
        elite_size = max(1, population_size // 10)
        elite_indices = np.argsort(fitness_scores)[:elite_size]
        for idx in elite_indices:
            new_population.append(population[idx].copy())
        
        # Generar resto
        while len(new_population) < population_size:
            # Selecci贸n por torneo
            parent1 = tournament_selection_fs(population, fitness_scores, 3)
            parent2 = tournament_selection_fs(population, fitness_scores, 3)
            
            # Cruce
            if random.random() < 0.8:
                child1, child2 = crossover_fs(parent1, parent2)
            else:
                child1, child2 = parent1.copy(), parent2.copy()
            
            # Mutaci贸n
            if random.random() < 0.3:
                child1 = mutate_fs(child1)
            if random.random() < 0.3:
                child2 = mutate_fs(child2)
            
            new_population.extend([child1, child2])
        
        population = new_population[:population_size]
    
    print(f"  Selecci贸n completada en {gen+1} generaciones")
    return best_individual, best_metrics


def evaluate_feature_subset(selection, X_train, y_train, X_test, y_test):
    """Eval煤a un subconjunto de features."""
    if np.sum(selection) == 0:
        return 1e6, {'mae': float('inf'), 'mse': float('inf')}

    try:
        X_train_sel = X_train[:, selection]
        X_test_sel = X_test[:, selection]

        model = LinearRegression()
        model.fit(X_train_sel, y_train)
        y_pred = model.predict(X_test_sel)

        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)

        # Penalizaci贸n por muchas features
        n_features = np.sum(selection)
        penalty = 0.001 * n_features

        return mse + penalty, {'mae': mae, 'mse': mse}
    except:
        return 1e6, {'mae': float('inf'), 'mse': float('inf')}


def tournament_selection_fs(population, fitness_scores, tournament_size):
    """Selecci贸n por torneo para feature selection."""
    tournament_idx = random.sample(range(len(population)), 
                                 min(tournament_size, len(population)))
    tournament_fitness = [fitness_scores[i] for i in tournament_idx]
    winner_idx = tournament_idx[np.argmin(tournament_fitness)]
    return population[winner_idx].copy()


def crossover_fs(parent1, parent2):
    """Cruce uniforme para feature selection."""
    child1 = parent1.copy()
    child2 = parent2.copy()
    
    # Cruce uniforme
    mask = np.random.rand(len(parent1)) < 0.5
    child1[mask] = parent2[mask]
    child2[mask] = parent1[mask]
    
    # Asegurar al menos 2 features
    if np.sum(child1) < 2:
        child1[random.randint(0, len(child1)-1)] = True
        child1[random.randint(0, len(child1)-1)] = True
    if np.sum(child2) < 2:
        child2[random.randint(0, len(child2)-1)] = True
        child2[random.randint(0, len(child2)-1)] = True
    
    return child1, child2


def mutate_fs(individual):
    """Mutaci贸n para feature selection."""
    mutated = individual.copy()
    
    # Flip 1-3 bits
    n_flips = random.randint(1, 3)
    for _ in range(n_flips):
        idx = random.randint(0, len(mutated) - 1)
        mutated[idx] = not mutated[idx]
    
    # Asegurar al menos 2 features
    if np.sum(mutated) < 2:
        mutated[random.randint(0, len(mutated)-1)] = True
        mutated[random.randint(0, len(mutated)-1)] = True
    
    return mutated

# Ejemplo de uso
if __name__ == "__main__":
    df = pd.read_csv('diabetes.csv')
    # CASO DIABETES
    X = df.drop('target', axis=1).values
    y = df['target'].values
    # CASO CALIFORNIA
    #X = df.drop('MedHouseVal', axis=1).values
    #y = df['MedHouseVal'].values
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Baseline
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    baseline = LinearRegression()
    baseline.fit(X_train_scaled, y_train)
    baseline_preds = baseline.predict(X_test_scaled)
    baseline_mae = mean_absolute_error(y_test, baseline_preds)
    baseline_mse = mean_squared_error(y_test, baseline_preds)
    #baseline_r2 = r2_score(y_test, baseline_preds)
    print(f"Baseline - MAE: {baseline_mae:.4f}")#, R虏: {baseline_r2:.4f}")
    print(f"Baseline - MSE: {baseline_mse:.4f}")

    # Programaci贸n gen茅tica
    gp_optimizer = EvolutionaryOptimizer(
        maxtime=1200  # 60 minutos
    )
    
    gp_optimizer.fit(X_train, y_train)
    #print(gp_optimizer.fitness_history_)
    
    y_pred = gp_optimizer.predict(X_test)
    
    gp_mae = mean_absolute_error(y_test, y_pred)
    gp_mse = mean_squared_error(y_test, y_pred)
    #gp_r2 = r2_score(y_test, y_pred)
    print(f"\nResultados:")
    print(f"Baseline: MAE={baseline_mae:.4f}")
    print(f"Baseline: MSE={baseline_mse:.4f}")

    print(f"GP:       MAE={gp_mae:.4f}")
    print(f"Mejora MAE:   {((baseline_mae - gp_mae) / baseline_mae * 100):+.2f}%")
    print(f"GP:       MSE={gp_mse:.4f}")
    print(f"Mejora MSE:   {((baseline_mse - gp_mse) / baseline_mse * 100):+.2f}%")
    
    # FEATURE SELECTION EVOLUTIVA
    print(f"\n{'='*70}")
    print(f"FEATURE SELECTION EVOLUTIVA")
    print(f"{'='*70}")
    
    # Dataset completo con todas las features
    X_train_full = gp_optimizer.transform(X_train)
    X_test_full = gp_optimizer.transform(X_test)
    
    print(f"Aplicando selecci贸n evolutiva sobre {X_train_full.shape[1]} features...")
    
    # Algoritmo evolutivo para selecci贸n de features
    best_selection, best_fs_metrics = evolutionary_feature_selection(
        X_train_full, y_train, X_test_full, y_test,
        population_size=30, generations=100
    )

    best_fs_mae = best_fs_metrics['mae']
    best_fs_mse = best_fs_metrics['mse']

    fs_improvement_mae = ((baseline_mae - best_fs_mae) / baseline_mae * 100)
    fs_improvement_mse = ((baseline_mse - best_fs_mse) / baseline_mse * 100)
    gp_vs_fs_improvement_mae = ((gp_mae - best_fs_mae) / gp_mae * 100)
    gp_vs_fs_improvement_mse = ((gp_mse - best_fs_mse) / gp_mse * 100)

    print(f"\n{'='*50}")
    print(f"RESULTADOS FINALES")
    print(f"{'='*50}")
    print(f"Baseline (solo originales):     MAE = {baseline_mae:.4f} | MSE = {baseline_mse:.4f}")
    print(f"GP (todas las features):        MAE = {gp_mae:.4f} | MSE = {gp_mse:.4f} | "
          f"Mejora MAE: {((baseline_mae - gp_mae) / baseline_mae * 100):+.2f}% | "
          f"Mejora MSE: {((baseline_mse - gp_mse) / baseline_mse * 100):+.2f}%")
    print(f"GP + Feature Selection:         MAE = {best_fs_mae:.4f} | MSE = {best_fs_mse:.4f} | "
          f"Mejora MAE: {fs_improvement_mae:+.2f}% | Mejora MSE: {fs_improvement_mse:+.2f}%")
    print()
    print(f"Mejora de Feature Selection sobre GP (MAE): {gp_vs_fs_improvement_mae:+.2f}%")
    print(f"Mejora de Feature Selection sobre GP (MSE): {gp_vs_fs_improvement_mse:+.2f}%")
    
    # Mostrar features seleccionadas
    selected_indices = np.where(best_selection)[0]
    print(f"\nFeatures seleccionadas ({len(selected_indices)}/{len(best_selection)}):")
    
    for idx in selected_indices:
        if idx < 10:
            print(f"  X{idx} (original)")
        else:
            tree_idx = idx - 10
            print(f"  {gp_optimizer.best_trees_[tree_idx].to_string()} (creada)")