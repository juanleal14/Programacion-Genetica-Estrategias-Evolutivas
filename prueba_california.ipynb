{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1710e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class GPNode:\n",
    "    \"\"\"Nodo de √°rbol de programaci√≥n gen√©tica.\"\"\"\n",
    "    \n",
    "    def __init__(self, value, children=None, node_type='terminal'):\n",
    "        self.value = value\n",
    "        self.children = children or []\n",
    "        self.node_type = node_type\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        \"\"\"Eval√∫a el nodo con los datos X.\"\"\"\n",
    "        if self.node_type == 'terminal':\n",
    "            if isinstance(self.value, int):\n",
    "                return X[:, self.value]\n",
    "            else:\n",
    "                return np.full(X.shape[0], self.value)\n",
    "        else:\n",
    "            if self.value == 'add':\n",
    "                return self.children[0].evaluate(X) + self.children[1].evaluate(X)\n",
    "            elif self.value == 'sub':\n",
    "                return self.children[0].evaluate(X) - self.children[1].evaluate(X)\n",
    "            elif self.value == 'mul':\n",
    "                return self.children[0].evaluate(X) * self.children[1].evaluate(X)\n",
    "            elif self.value == 'div':\n",
    "                right = self.children[1].evaluate(X)\n",
    "                return self.children[0].evaluate(X) / (right + 1e-6)\n",
    "            elif self.value == 'sqrt':\n",
    "                return np.sqrt(np.abs(self.children[0].evaluate(X)) + 1e-6)\n",
    "            elif self.value == 'square':\n",
    "                val = self.children[0].evaluate(X)\n",
    "                return np.clip(val, -100, 100) ** 2\n",
    "            elif self.value == 'log':\n",
    "                return np.log(np.abs(self.children[0].evaluate(X)) + 1)\n",
    "            #elif self.value == 'abs':\n",
    "            #    return np.abs(self.children[0].evaluate(X))\n",
    "            elif self.value == 'sin':\n",
    "                return np.sin(self.children[0].evaluate(X))\n",
    "            elif self.value == 'cos':\n",
    "                return np.cos(self.children[0].evaluate(X))\n",
    "            elif self.value == 'tanh':\n",
    "                return np.tanh(self.children[0].evaluate(X))\n",
    "    \n",
    "    def copy(self):\n",
    "        \"\"\"Copia profunda del nodo.\"\"\"\n",
    "        return GPNode(\n",
    "            self.value,\n",
    "            [child.copy() for child in self.children],\n",
    "            self.node_type\n",
    "        )\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\"Tama√±o del √°rbol (n√∫mero de nodos).\"\"\"\n",
    "        return 1 + sum(child.size() for child in self.children)\n",
    "    \n",
    "    def depth(self):\n",
    "        \"\"\"Profundidad del √°rbol.\"\"\"\n",
    "        if not self.children:\n",
    "            return 1\n",
    "        return 1 + max(child.depth() for child in self.children)\n",
    "    \n",
    "    def to_string(self):\n",
    "        \"\"\"Representaci√≥n en string del √°rbol.\"\"\"\n",
    "        if self.node_type == 'terminal':\n",
    "            if isinstance(self.value, int):\n",
    "                return f\"X{self.value}\"\n",
    "            else:\n",
    "                return f\"{self.value:.3f}\"\n",
    "        else:\n",
    "            if len(self.children) == 1:\n",
    "                return f\"{self.value}({self.children[0].to_string()})\"\n",
    "            else:\n",
    "                return f\"({self.children[0].to_string()} {self.value} {self.children[1].to_string()})\"\n",
    "\n",
    "\n",
    "class EvolutionaryOptimizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Optimizador de programaci√≥n gen√©tica para crear features.\"\"\"\n",
    "    \n",
    "    def __init__(self, maxtime=1200, population_size=100, n_features_to_create=4,\n",
    "                 mutation_prob=0.2, crossover_prob=0.8, tournament_size=5,\n",
    "                 max_depth=5, elite_size=0.1, apply_feature_selection=True,\n",
    "                 evaluation_model='ridge'):\n",
    "        self.maxtime = maxtime\n",
    "        self.population_size = population_size\n",
    "        self.n_features_to_create = n_features_to_create\n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.crossover_prob = crossover_prob\n",
    "        self.tournament_size = tournament_size\n",
    "        self.max_depth = max_depth\n",
    "        self.elite_size = int(population_size * elite_size)\n",
    "        self.apply_feature_selection = apply_feature_selection\n",
    "        self.evaluation_model = evaluation_model  # 'ridge' o 'linear'\n",
    "        \n",
    "        # Funciones disponibles\n",
    "        self.functions = {\n",
    "            'add': 2, 'sub': 2, 'mul': 2, 'div': 2,\n",
    "            'sqrt': 1, 'square': 1, 'log': 1, 'sin': 1, 'cos': 1, 'tanh': 1\n",
    "        }\n",
    "        \n",
    "        self.best_trees_ = []\n",
    "        self.best_fitness_ = float('inf')\n",
    "        self.fitness_history_ = []\n",
    "        self.best_metrics_ = {'mae': None, 'mse': None}\n",
    "        self.feature_selection_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena usando programaci√≥n gen√©tica y selecci√≥n de features.\"\"\"\n",
    "        total_time = self.maxtime\n",
    "        gp_time = total_time * 0.7  # 70% para GP\n",
    "        fs_time = total_time * 0.3  # 30% para Feature Selection\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        \n",
    "        # Divisi√≥n train/validation para early stopping\n",
    "        n_val = int(len(X) * 0.2)\n",
    "        indices = np.random.permutation(len(X))\n",
    "        val_idx, train_idx = indices[:n_val], indices[n_val:]\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        self.scaler_ = RobustScaler()\n",
    "        X_train_scaled = self.scaler_.fit_transform(X_train)\n",
    "        X_val_scaled = self.scaler_.transform(X_val)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PROGRAMACI√ìN GEN√âTICA\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Poblaci√≥n: {self.population_size} | Features a crear: {self.n_features_to_create}\")\n",
    "        print(f\"Profundidad m√°xima: {self.max_depth}\")\n",
    "        print(f\"Modelo de evaluaci√≥n: {self.evaluation_model.upper()}\")\n",
    "        print(f\"Tiempo asignado GP: {gp_time/60:.1f}min ({gp_time}s)\")\n",
    "        print(f\"Tiempo asignado FS: {fs_time/60:.1f}min ({fs_time}s)\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Inicializar poblaci√≥n de √°rboles\n",
    "        population = [self._create_random_tree() for _ in range(self.population_size)]\n",
    "        \n",
    "        generation = 0\n",
    "        gp_early_stop = 0\n",
    "        best_val_fitness = float('inf')\n",
    "        gp_start = time.time()\n",
    "        \n",
    "        # FASE 1: PROGRAMACI√ìN GEN√âTICA (70% del tiempo)\n",
    "        while (time.time() - gp_start) < gp_time:\n",
    "            generation += 1\n",
    "            \n",
    "            # Evaluar poblaci√≥n en train y validaci√≥n\n",
    "            train_fitness = []\n",
    "            val_fitness = []\n",
    "            for individual in population:\n",
    "                train_fit, _ = self._evaluate_individual(individual, X_train_scaled, y_train)\n",
    "                val_fit, val_metrics = self._evaluate_individual(individual, X_val_scaled, y_val)\n",
    "                train_fitness.append(train_fit)\n",
    "                val_fitness.append(val_fit)\n",
    "\n",
    "            # Actualizar mejor basado en VALIDACI√ìN\n",
    "            best_val_idx = np.argmin(val_fitness)\n",
    "            if val_fitness[best_val_idx] < best_val_fitness:\n",
    "                best_val_fitness = val_fitness[best_val_idx]\n",
    "                self.best_fitness_ = train_fitness[best_val_idx]\n",
    "                self.best_trees_ = [tree.copy() for tree in population[best_val_idx]]\n",
    "                self.best_metrics_ = val_metrics\n",
    "                gp_early_stop = 0\n",
    "                print(f\"Gen {generation} - MEJORA! Val: {best_val_fitness:.4f} | Train: {self.best_fitness_:.4f}\")\n",
    "            else:\n",
    "                gp_early_stop += 1\n",
    "            \n",
    "            # Early stopping para GP\n",
    "            if gp_early_stop >= 100:  # 100 generaciones sin mejora en validaci√≥n\n",
    "                print(f\"GP Early stopping en generaci√≥n {generation}\")\n",
    "                break\n",
    "            \n",
    "            # CAMBIO DE RAMA cada 200 generaciones de estancamiento\n",
    "            #if stagnation > 0 and stagnation % 200 == 0:\n",
    "            #    print(f\"\\nüîÑ CAMBIO DE RAMA en generaci√≥n {generation}\")\n",
    "            #    print(f\"   Estancamiento: {stagnation} generaciones\")\n",
    "            #    print(f\"   Diversificando poblaci√≥n...\")\n",
    "            #    \n",
    "            #    elite_size_restart = max(1, self.population_size // 5)\n",
    "            #    elite_indices = np.argsort(fitness_scores)[:elite_size_restart]\n",
    "            #    \n",
    "            #    new_population_restart = []\n",
    "            #    \n",
    "            #    for idx in elite_indices:\n",
    "            #        new_population_restart.append([tree.copy() for tree in population[idx]])\n",
    "            #    \n",
    "            #    while len(new_population_restart) < self.population_size:\n",
    "            #        new_individual = []\n",
    "            #        for _ in range(self.n_features_to_create):\n",
    "            #            depth = random.randint(3, self.max_depth + 1)\n",
    "            #            tree = self._generate_tree(max_depth=depth)\n",
    "            #            new_individual.append(tree)\n",
    "            #        new_population_restart.append(new_individual)\n",
    "            #    \n",
    "            #    population = new_population_restart\n",
    "            #    print(f\"   Nueva poblaci√≥n creada con {len(population)} individuos\")\n",
    "            \n",
    "            self.fitness_history_.append(self.best_fitness_)\n",
    "            \n",
    "            # Nueva generaci√≥n basada en validaci√≥n\n",
    "            new_population = []\n",
    "            \n",
    "            # Elitismo basado en validaci√≥n\n",
    "            elite_indices = np.argsort(val_fitness)[:self.elite_size]\n",
    "            for idx in elite_indices:\n",
    "                new_population.append([tree.copy() for tree in population[idx]])\n",
    "            \n",
    "            # Generar resto\n",
    "            while len(new_population) < self.population_size:\n",
    "                parent1 = self._tournament_selection(population, val_fitness)\n",
    "                parent2 = self._tournament_selection(population, val_fitness)\n",
    "                \n",
    "                if random.random() < self.crossover_prob:\n",
    "                    child1, child2 = self._crossover_trees(parent1, parent2)\n",
    "                else:\n",
    "                    child1 = [tree.copy() for tree in parent1]\n",
    "                    child2 = [tree.copy() for tree in parent2]\n",
    "                \n",
    "                if random.random() < self.mutation_prob:\n",
    "                    child1 = self._mutate_trees(child1)\n",
    "                if random.random() < self.mutation_prob:\n",
    "                    child2 = self._mutate_trees(child2)\n",
    "                \n",
    "                new_population.extend([child1, child2])\n",
    "            \n",
    "            population = new_population[:self.population_size]\n",
    "            \n",
    "            if generation % 50 == 0:\n",
    "                elapsed = time.time() - gp_start\n",
    "                print(f\"Gen {generation} | Val: {best_val_fitness:.4f} | Train: {self.best_fitness_:.4f} | \" +\n",
    "                      f\"Tiempo GP: {elapsed/60:.1f}min | Early stop: {gp_early_stop}\")\n",
    "        \n",
    "        gp_elapsed = time.time() - gp_start\n",
    "        print(f\"\\nProgramaci√≥n Gen√©tica completada en {generation} generaciones ({gp_elapsed/60:.1f}min)\")\n",
    "        if (\n",
    "            self.best_metrics_['mse'] is not None\n",
    "            and self.best_metrics_['mae'] is not None\n",
    "            and np.isfinite(self.best_metrics_['mse'])\n",
    "            and np.isfinite(self.best_metrics_['mae'])\n",
    "        ):\n",
    "            print(f\"Mejor MSE: {self.best_metrics_['mse']:.4f} | Mejor MAE: {self.best_metrics_['mae']:.4f}\")\n",
    "        print(f\"Mejores √°rboles encontrados:\")\n",
    "        for i, tree in enumerate(self.best_trees_):\n",
    "            print(f\"  {i+1}: {tree.to_string()}\")\n",
    "\n",
    "        # FASE 2: FEATURE SELECTION EVOLUTIVA (30% del tiempo)\n",
    "        if self.apply_feature_selection:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FEATURE SELECTION EVOLUTIVA\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Transformar el dataset completo con las features generadas\n",
    "            X_transformed = self._transform_without_selection(X)\n",
    "            \n",
    "            print(f\"Aplicando selecci√≥n evolutiva sobre {X_transformed.shape[1]} features...\")\n",
    "            print(f\"Usando cross-validation (5 folds) para m√°s robustez...\")\n",
    "            print(f\"Tiempo m√°ximo: {fs_time/60:.1f}min ({fs_time}s)\")\n",
    "            \n",
    "            # Aplicar selecci√≥n evolutiva usando CV con l√≠mite de tiempo\n",
    "            fs_start = time.time()\n",
    "            self.feature_selection_, fs_metrics = self._evolutionary_feature_selection_cv(\n",
    "                X_transformed, y,\n",
    "                population_size=30,\n",
    "                max_time=fs_time  # Pasar l√≠mite de tiempo\n",
    "            )\n",
    "            fs_elapsed = time.time() - fs_start\n",
    "            \n",
    "            n_selected = np.sum(self.feature_selection_)\n",
    "            print(f\"\\n‚úì Selecci√≥n completada: {n_selected}/{len(self.feature_selection_)} features seleccionadas\")\n",
    "            print(f\"  Mejor MAE (CV): {fs_metrics['mae']:.4f}\")\n",
    "            print(f\"  Mejor MSE (CV): {fs_metrics['mse']:.4f}\")\n",
    "            print(f\"  Tiempo usado: {fs_elapsed/60:.1f}min\")\n",
    "\n",
    "        total_elapsed = time.time() - start_time\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ENTRENAMIENTO COMPLETADO\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Tiempo total: {total_elapsed/60:.1f}min\")\n",
    "        print(f\"  - Programaci√≥n Gen√©tica: {gp_elapsed/60:.1f}min ({gp_elapsed/total_elapsed*100:.1f}%)\")\n",
    "        if self.apply_feature_selection:\n",
    "            print(f\"  - Feature Selection: {fs_elapsed/60:.1f}min ({fs_elapsed/total_elapsed*100:.1f}%)\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _transform_without_selection(self, X):\n",
    "        \"\"\"Transforma datos sin aplicar selecci√≥n de features.\"\"\"\n",
    "        if not self.best_trees_:\n",
    "            raise ValueError(\"No entrenado\")\n",
    "        \n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        \n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        X_new = X_scaled.copy()\n",
    "        \n",
    "        for tree in self.best_trees_:\n",
    "            try:\n",
    "                new_feature = tree.evaluate(X_scaled)\n",
    "                new_feature = np.nan_to_num(new_feature, nan=0.0, posinf=100.0, neginf=-100.0)\n",
    "                new_feature = np.clip(new_feature, -1000, 1000)\n",
    "                X_new = np.column_stack([X_new, new_feature])\n",
    "            except:\n",
    "                X_new = np.column_stack([X_new, np.zeros(X_scaled.shape[0])])\n",
    "        \n",
    "        return X_new\n",
    "    \n",
    "    def _get_evaluation_model(self):\n",
    "        \"\"\"Retorna el modelo a usar para evaluaci√≥n.\"\"\"\n",
    "        if self.evaluation_model == 'ridge':\n",
    "            # Ridge con regularizaci√≥n adaptativa\n",
    "            return Ridge(alpha=1.0, random_state=42)\n",
    "        else:\n",
    "            return LinearRegression()\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transforma datos usando los mejores √°rboles y aplica selecci√≥n de features.\"\"\"\n",
    "        X_transformed = self._transform_without_selection(X)\n",
    "        \n",
    "        # Aplicar selecci√≥n de features si existe\n",
    "        if self.feature_selection_ is not None:\n",
    "            X_transformed = X_transformed[:, self.feature_selection_]\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def _create_random_tree(self):\n",
    "        \"\"\"Crea conjunto aleatorio de √°rboles.\"\"\"\n",
    "        trees = []\n",
    "        for _ in range(self.n_features_to_create):\n",
    "            tree = self._generate_tree(max_depth=random.randint(2, self.max_depth))\n",
    "            trees.append(tree)\n",
    "        return trees\n",
    "    \n",
    "    def _generate_tree(self, max_depth):\n",
    "        \"\"\"Genera un √°rbol aleatorio.\"\"\"\n",
    "        if max_depth <= 1 or random.random() < 0.3:\n",
    "            if random.random() < 0.8:\n",
    "                return GPNode(random.randint(0, self.n_features_in_ - 1), node_type='terminal')\n",
    "            else:\n",
    "                return GPNode(random.uniform(-5, 5), node_type='terminal')\n",
    "        else:\n",
    "            func_name = random.choice(list(self.functions.keys()))\n",
    "            arity = self.functions[func_name]\n",
    "            children = [self._generate_tree(max_depth - 1) for _ in range(arity)]\n",
    "            return GPNode(func_name, children, node_type='function')\n",
    "    \n",
    "    def _evaluate_individual(self, trees, X, y):\n",
    "        \"\"\"Eval√∫a un individuo (conjunto de √°rboles).\"\"\"\n",
    "        try:\n",
    "            X_new = X.copy()\n",
    "            for tree in trees:\n",
    "                try:\n",
    "                    new_feature = tree.evaluate(X)\n",
    "                    new_feature = np.nan_to_num(new_feature, nan=0.0, posinf=100.0, neginf=-100.0)\n",
    "                    new_feature = np.clip(new_feature, -1000, 1000)\n",
    "                    X_new = np.column_stack([X_new, new_feature])\n",
    "                except:\n",
    "                    X_new = np.column_stack([X_new, np.zeros(X.shape[0])])\n",
    "\n",
    "            if np.any(np.abs(X_new) > 1e6) or np.any(np.std(X_new, axis=0) < 1e-10):\n",
    "                return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "\n",
    "            # Usar el modelo configurado (Ridge o Linear) con 5-fold CV\n",
    "            model = self._get_evaluation_model()\n",
    "            cv_results = cross_validate(\n",
    "                model,\n",
    "                X_new,\n",
    "                y,\n",
    "                cv=3,  # 5-fold para m√°s robustez\n",
    "                scoring={'mae': 'neg_mean_absolute_error', 'mse': 'neg_mean_squared_error'},\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            if (np.any(np.isnan(cv_results['test_mae'])) or\n",
    "                    np.any(np.isnan(cv_results['test_mse']))):\n",
    "                return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "\n",
    "            mae = -cv_results['test_mae'].mean()\n",
    "            mse = -cv_results['test_mse'].mean()\n",
    "\n",
    "            complexity = sum(tree.size() for tree in trees)\n",
    "            penalty = 0.001 * complexity\n",
    "            fitness = mse + penalty\n",
    "\n",
    "            return fitness, {'mae': mae, 'mse': mse}\n",
    "\n",
    "        except:\n",
    "            return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "    \n",
    "    def _tournament_selection(self, population, fitness_scores):\n",
    "        \"\"\"Selecci√≥n por torneo.\"\"\"\n",
    "        tournament_idx = random.sample(range(len(population)), \n",
    "                                     min(self.tournament_size, len(population)))\n",
    "        tournament_fitness = [fitness_scores[i] for i in tournament_idx]\n",
    "        winner_idx = tournament_idx[np.argmin(tournament_fitness)]\n",
    "        return [tree.copy() for tree in population[winner_idx]]\n",
    "    \n",
    "    def _crossover_trees(self, parent1, parent2):\n",
    "        \"\"\"Cruce entre conjuntos de √°rboles.\"\"\"\n",
    "        child1 = [tree.copy() for tree in parent1]\n",
    "        child2 = [tree.copy() for tree in parent2]\n",
    "        \n",
    "        n_swap = random.randint(1, min(len(child1), len(child2)) // 2)\n",
    "        indices = random.sample(range(min(len(child1), len(child2))), n_swap)\n",
    "        \n",
    "        for idx in indices:\n",
    "            child1[idx], child2[idx] = child2[idx].copy(), child1[idx].copy()\n",
    "        \n",
    "        for i in range(min(len(child1), len(child2))):\n",
    "            if random.random() < 0.3:\n",
    "                child1[i], child2[i] = self._crossover_single_tree(child1[i], child2[i])\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def _crossover_single_tree(self, tree1, tree2):\n",
    "        \"\"\"Cruce entre dos √°rboles individuales.\"\"\"\n",
    "        if tree1.children and tree2.children and random.random() < 0.7:\n",
    "            idx1 = random.randint(0, len(tree1.children) - 1)\n",
    "            idx2 = random.randint(0, len(tree2.children) - 1)\n",
    "            tree1.children[idx1], tree2.children[idx2] = tree2.children[idx2].copy(), tree1.children[idx1].copy()\n",
    "        \n",
    "        return tree1, tree2\n",
    "    \n",
    "    def _mutate_trees(self, trees):\n",
    "        \"\"\"Mutaci√≥n de conjunto de √°rboles.\"\"\"\n",
    "        mutated = [tree.copy() for tree in trees]\n",
    "        \n",
    "        for i, tree in enumerate(mutated):\n",
    "            if random.random() < 0.3:\n",
    "                mutated[i] = self._mutate_single_tree(tree)\n",
    "        \n",
    "        if random.random() < 0.1:\n",
    "            idx = random.randint(0, len(mutated) - 1)\n",
    "            mutated[idx] = self._generate_tree(max_depth=random.randint(2, self.max_depth))\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def _mutate_single_tree(self, tree):\n",
    "        \"\"\"Mutaci√≥n de un √°rbol individual.\"\"\"\n",
    "        mutated = tree.copy()\n",
    "        \n",
    "        if random.random() < 0.5 and mutated.children:\n",
    "            mutated.value = random.choice(list(self.functions.keys()))\n",
    "            required_arity = self.functions[mutated.value]\n",
    "            while len(mutated.children) < required_arity:\n",
    "                mutated.children.append(self._generate_tree(max_depth=2))\n",
    "            mutated.children = mutated.children[:required_arity]\n",
    "        \n",
    "        elif mutated.node_type == 'terminal':\n",
    "            if isinstance(mutated.value, int):\n",
    "                mutated.value = random.randint(0, self.n_features_in_ - 1)\n",
    "            else:\n",
    "                mutated.value = random.uniform(-5, 5)\n",
    "        \n",
    "        for child in mutated.children:\n",
    "            if random.random() < 0.2:\n",
    "                child = self._mutate_single_tree(child)\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def _evolutionary_feature_selection_cv(self, X_full, y_full, \n",
    "                                       population_size=50, max_time=None):  # Aumentar poblaci√≥n\n",
    "        \"\"\"Selecci√≥n evolutiva de features usando Cross-Validation con l√≠mite de tiempo.\"\"\"\n",
    "        n_features = X_full.shape[1]\n",
    "        \n",
    "        # Divisi√≥n train/validation para FS\n",
    "        n_val = int(len(X_full) * 0.2)\n",
    "        indices = np.random.permutation(len(X_full))\n",
    "        val_idx, train_idx = indices[:n_val], indices[n_val:]\n",
    "        \n",
    "        X_fs_train, X_fs_val = X_full[train_idx], X_full[val_idx]\n",
    "        y_fs_train, y_fs_val = y_full[train_idx], y_full[val_idx]\n",
    "        \n",
    "        # Inicializar poblaci√≥n con M√ÅS DIVERSIDAD\n",
    "        population = []\n",
    "        for _ in range(population_size):\n",
    "            individual = np.zeros(n_features, dtype=bool)\n",
    "            # Variar m√°s el n√∫mero de features seleccionadas\n",
    "            n_selected = random.randint(2, min(n_features-1, max(8, n_features//2)))\n",
    "            selected_idx = random.sample(range(n_features), n_selected)\n",
    "            individual[selected_idx] = True\n",
    "            population.append(individual)\n",
    "        \n",
    "        best_val_fitness = float('inf')\n",
    "        best_individual = None\n",
    "        best_metrics = {'mae': None, 'mse': None}\n",
    "        fs_early_stop = 0\n",
    "        \n",
    "        fs_start = time.time()\n",
    "        gen = 0\n",
    "        \n",
    "        while True:\n",
    "            gen += 1\n",
    "            \n",
    "            # Verificar l√≠mite de tiempo si existe\n",
    "            if max_time is not None and (time.time() - fs_start) >= max_time:\n",
    "                print(f\"  L√≠mite de tiempo alcanzado ({max_time}s)\")\n",
    "                break\n",
    "            \n",
    "            train_fitness = []\n",
    "            val_fitness = []\n",
    "            for individual in population:\n",
    "                train_fit, _ = self._evaluate_feature_subset_simple(individual, X_fs_train, y_fs_train)\n",
    "                val_fit, val_metrics = self._evaluate_feature_subset_simple(individual, X_fs_val, y_fs_val)\n",
    "                train_fitness.append(train_fit)\n",
    "                val_fitness.append(val_fit)\n",
    "\n",
    "            current_best_idx = np.argmin(val_fitness)\n",
    "            if val_fitness[current_best_idx] < best_val_fitness:\n",
    "                best_val_fitness = val_fitness[current_best_idx]\n",
    "                best_individual = population[current_best_idx].copy()\n",
    "                best_metrics = val_metrics\n",
    "                fs_early_stop = 0\n",
    "                if gen % 20 == 0 or gen < 10:\n",
    "                    n_selected = np.sum(best_individual)\n",
    "                    elapsed = time.time() - fs_start\n",
    "                    print(f\"  Gen {gen}: Val MSE = {best_metrics['mse']:.4f} | Features: {n_selected} | Tiempo: {elapsed:.1f}s\")\n",
    "            else:\n",
    "                fs_early_stop += 1\n",
    "            \n",
    "            # CAMBIO PRINCIPAL: Early stopping m√°s permisivo O usar tiempo completo\n",
    "            max_early_stop = max(100, population_size * 2)  # M√≠nimo 100 generaciones\n",
    "            if fs_early_stop >= max_early_stop:\n",
    "                print(f\"  FS Early stopping en generaci√≥n {gen}\")\n",
    "                break\n",
    "            \n",
    "            # Nueva generaci√≥n basada en validaci√≥n\n",
    "            new_population = []\n",
    "            \n",
    "            elite_size = max(2, population_size // 8)  # M√°s elite\n",
    "            elite_indices = np.argsort(val_fitness)[:elite_size]\n",
    "            for idx in elite_indices:\n",
    "                new_population.append(population[idx].copy())\n",
    "            \n",
    "            while len(new_population) < population_size:\n",
    "                parent1 = self._tournament_selection_fs(population, val_fitness, 3)\n",
    "                parent2 = self._tournament_selection_fs(population, val_fitness, 3)\n",
    "                \n",
    "                if random.random() < 0.8:\n",
    "                    child1, child2 = self._crossover_fs(parent1, parent2)\n",
    "                else:\n",
    "                    child1, child2 = parent1.copy(), parent2.copy()\n",
    "                \n",
    "                if random.random() < 0.4:  # M√°s mutaci√≥n\n",
    "                    child1 = self._mutate_fs(child1)\n",
    "                if random.random() < 0.4:\n",
    "                    child2 = self._mutate_fs(child2)\n",
    "                \n",
    "                new_population.extend([child1, child2])\n",
    "            \n",
    "            population = new_population[:population_size]\n",
    "        \n",
    "        print(f\"  Selecci√≥n completada en {gen} generaciones\")\n",
    "        return best_individual, best_metrics\n",
    "    \n",
    "    def _evaluate_feature_subset_simple(self, selection, X, y):\n",
    "        \"\"\"Eval√∫a subconjunto de features sin CV (m√°s r√°pido).\"\"\"\n",
    "        if np.sum(selection) == 0:\n",
    "            return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "\n",
    "        try:\n",
    "            X_selected = X[:, selection]\n",
    "            model = self._get_evaluation_model()\n",
    "            model.fit(X_selected, y)\n",
    "            y_pred = model.predict(X_selected)\n",
    "            \n",
    "            mae = mean_absolute_error(y, y_pred)\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            \n",
    "            # Penalizaci√≥n por muchas features\n",
    "            n_features = np.sum(selection)\n",
    "            penalty = 0.01 * n_features\n",
    "\n",
    "            return mse + penalty, {'mae': mae, 'mse': mse}\n",
    "        except:\n",
    "            return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "    \n",
    "    def _evaluate_feature_subset_cv(self, selection, X_full, y_full, cv_folds=3):\n",
    "        \"\"\"Eval√∫a un subconjunto de features usando Cross-Validation.\"\"\"\n",
    "        if np.sum(selection) == 0:\n",
    "            return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "\n",
    "        try:\n",
    "            X_selected = X_full[:, selection]\n",
    "\n",
    "            # Usar el modelo configurado (Ridge o Linear)\n",
    "            model = self._get_evaluation_model()\n",
    "            \n",
    "            # Cross-validation con m√°s folds para m√°s robustez\n",
    "            cv_results = cross_validate(\n",
    "                model,\n",
    "                X_selected,\n",
    "                y_full,\n",
    "                cv=cv_folds,\n",
    "                scoring={'mae': 'neg_mean_absolute_error', 'mse': 'neg_mean_squared_error'},\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            if (np.any(np.isnan(cv_results['test_mae'])) or\n",
    "                    np.any(np.isnan(cv_results['test_mse']))):\n",
    "                return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "\n",
    "            mae = -cv_results['test_mae'].mean()\n",
    "            mse = -cv_results['test_mse'].mean()\n",
    "\n",
    "            # Penalizaci√≥n m√°s fuerte por muchas features (reducir overfitting)\n",
    "            n_features = np.sum(selection)\n",
    "            penalty = 0.01 * n_features  # 10x m√°s penalizaci√≥n\n",
    "\n",
    "            return mse + penalty, {'mae': mae, 'mse': mse}\n",
    "        except:\n",
    "            return 1e6, {'mae': float('inf'), 'mse': float('inf')}\n",
    "    \n",
    "    def _tournament_selection_fs(self, population, fitness_scores, tournament_size):\n",
    "        \"\"\"Selecci√≥n por torneo para feature selection.\"\"\"\n",
    "        tournament_idx = random.sample(range(len(population)), \n",
    "                                     min(tournament_size, len(population)))\n",
    "        tournament_fitness = [fitness_scores[i] for i in tournament_idx]\n",
    "        winner_idx = tournament_idx[np.argmin(tournament_fitness)]\n",
    "        return population[winner_idx].copy()\n",
    "    \n",
    "    def _crossover_fs(self, parent1, parent2):\n",
    "        \"\"\"Cruce uniforme para feature selection.\"\"\"\n",
    "        child1 = parent1.copy()\n",
    "        child2 = parent2.copy()\n",
    "        \n",
    "        mask = np.random.rand(len(parent1)) < 0.5\n",
    "        child1[mask] = parent2[mask]\n",
    "        child2[mask] = parent1[mask]\n",
    "        \n",
    "        if np.sum(child1) < 2:\n",
    "            child1[random.randint(0, len(child1)-1)] = True\n",
    "            child1[random.randint(0, len(child1)-1)] = True\n",
    "        if np.sum(child2) < 2:\n",
    "            child2[random.randint(0, len(child2)-1)] = True\n",
    "            child2[random.randint(0, len(child2)-1)] = True\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def _mutate_fs(self, individual):\n",
    "        \"\"\"Mutaci√≥n para feature selection.\"\"\"\n",
    "        mutated = individual.copy()\n",
    "        \n",
    "        n_flips = random.randint(1, 3)\n",
    "        for _ in range(n_flips):\n",
    "            idx = random.randint(0, len(mutated) - 1)\n",
    "            mutated[idx] = not mutated[idx]\n",
    "        \n",
    "        if np.sum(mutated) < 2:\n",
    "            mutated[random.randint(0, len(mutated)-1)] = True\n",
    "            mutated[random.randint(0, len(mutated)-1)] = True\n",
    "        \n",
    "        return mutated\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a01afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUACI√ìN DEL SISTEMA\n",
      "======================================================================\n",
      "Dataset: 20640 instancias, 8 features\n",
      "Train: 16512 | Test: 4128\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BASELINE (Sin Optimizaci√≥n)\n",
      "======================================================================\n",
      "MAE: 0.5332\n",
      "MSE: 0.5558\n",
      "Features utilizadas: 8\n",
      "\n",
      "======================================================================\n",
      "OPTIMIZACI√ìN EVOLUTIVA\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PROGRAMACI√ìN GEN√âTICA\n",
      "======================================================================\n",
      "Poblaci√≥n: 100 | Features a crear: 4\n",
      "Profundidad m√°xima: 5\n",
      "Modelo de evaluaci√≥n: RIDGE\n",
      "Tiempo asignado GP: 14.0min (840.0s)\n",
      "Tiempo asignado FS: 6.0min (360.0s)\n",
      "======================================================================\n",
      "\n",
      "Gen 1 - MEJORA! Val: 0.4337 | Train: 0.5372\n",
      "Gen 2 - MEJORA! Val: 0.4325 | Train: 0.4434\n",
      "Gen 3 - MEJORA! Val: 0.4167 | Train: 0.5593\n",
      "Gen 5 - MEJORA! Val: 0.4142 | Train: 0.5100\n",
      "Gen 8 - MEJORA! Val: 0.4094 | Train: 0.4893\n",
      "Gen 14 - MEJORA! Val: 0.4089 | Train: 0.5235\n",
      "Gen 34 - MEJORA! Val: 0.4034 | Train: 3.1729\n",
      "Gen 50 | Val: 0.4034 | Train: 3.1729 | Tiempo GP: 11.6min | Early stop: 16\n",
      "\n",
      "Programaci√≥n Gen√©tica completada en 61 generaciones (14.2min)\n",
      "Mejor MSE: 0.3999 | Mejor MAE: 0.4616\n",
      "Mejores √°rboles encontrados:\n",
      "  1: (X5 div tanh(X5))\n",
      "  2: square(X0)\n",
      "  3: sqrt(X7)\n",
      "  4: tanh(cos(X0))\n",
      "\n",
      "======================================================================\n",
      "FEATURE SELECTION EVOLUTIVA\n",
      "======================================================================\n",
      "Aplicando selecci√≥n evolutiva sobre 12 features...\n",
      "Usando cross-validation (5 folds) para m√°s robustez...\n",
      "Tiempo m√°ximo: 6.0min (360.0s)\n",
      "  Gen 1: Val MSE = 1.2980 | Features: 5 | Tiempo: 0.1s\n",
      "  Gen 2: Val MSE = 0.5251 | Features: 5 | Tiempo: 0.2s\n",
      "  Gen 3: Val MSE = 0.5247 | Features: 8 | Tiempo: 0.3s\n",
      "  Gen 4: Val MSE = 0.5112 | Features: 7 | Tiempo: 0.4s\n",
      "  Gen 5: Val MSE = 0.5251 | Features: 7 | Tiempo: 0.5s\n",
      "  Gen 6: Val MSE = 0.4871 | Features: 9 | Tiempo: 0.6s\n",
      "  Gen 7: Val MSE = 0.6208 | Features: 8 | Tiempo: 0.8s\n",
      "  FS Early stopping en generaci√≥n 1010\n",
      "  Selecci√≥n completada en 1010 generaciones\n",
      "\n",
      "‚úì Selecci√≥n completada: 7/12 features seleccionadas\n",
      "  Mejor MAE (CV): 0.4668\n",
      "  Mejor MSE (CV): 0.4262\n",
      "  Tiempo usado: 2.0min\n",
      "\n",
      "======================================================================\n",
      "ENTRENAMIENTO COMPLETADO\n",
      "======================================================================\n",
      "Tiempo total: 16.1min\n",
      "  - Programaci√≥n Gen√©tica: 14.2min (87.8%)\n",
      "  - Feature Selection: 2.0min (12.2%)\n",
      "======================================================================\n",
      "Features seleccionadas: 7/12\n",
      "\n",
      "Features seleccionadas:\n",
      "  X0 (original)\n",
      "  X5 (original)\n",
      "  X6 (original)\n",
      "  X7 (original)\n",
      "  (X5 div tanh(X5)) (generada)\n",
      "  sqrt(X7) (generada)\n",
      "  tanh(cos(X0)) (generada)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN - EJEMPLO DE USO Y EVALUACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Cargar dataset\n",
    "    df = pd.read_csv('california.csv')\n",
    "    \n",
    "    # CASO DIABETES\n",
    "    #X = df.drop('target', axis=1).values\n",
    "    #y = df['target'].values\n",
    "    \n",
    "    # CASO CALIFORNIA (descomentar si se usa otro dataset)\n",
    "    X = df.drop('MedHouseVal', axis=1).values\n",
    "    y = df['MedHouseVal'].values\n",
    "    \n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUACI√ìN DEL SISTEMA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Dataset: {X.shape[0]} instancias, {X.shape[1]} features\")\n",
    "    print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # BASELINE: Modelo sin optimizaci√≥n\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BASELINE (Sin Optimizaci√≥n)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    baseline = Ridge(alpha=1.0, random_state=42)\n",
    "    baseline.fit(X_train, y_train)\n",
    "    baseline_preds = baseline.predict(X_test)\n",
    "    \n",
    "    baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "    baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "    \n",
    "    print(f\"MAE: {baseline_mae:.4f}\")\n",
    "    print(f\"MSE: {baseline_mse:.4f}\")\n",
    "    print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # OPTIMIZACI√ìN CON PROGRAMACI√ìN GEN√âTICA + FEATURE SELECTION\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"OPTIMIZACI√ìN EVOLUTIVA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Crear optimizador\n",
    "    gp_optimizer = EvolutionaryOptimizer(\n",
    "        maxtime=1200,  # 20 minutos (ajusta seg√∫n necesites)\n",
    "    )\n",
    "    \n",
    "    # Entrenar el optimizador (aprende transformaciones)\n",
    "    gp_optimizer.fit(X_train, y_train)\n",
    "    \n",
    "    # Transformar los datos (aplicar las transformaciones aprendidas)\n",
    "    X_train_optimized = gp_optimizer.transform(X_train)\n",
    "    X_test_optimized = gp_optimizer.transform(X_test)\n",
    "    \n",
    "    if gp_optimizer.feature_selection_ is not None:\n",
    "            n_selected = np.sum(gp_optimizer.feature_selection_)\n",
    "            n_total = len(gp_optimizer.feature_selection_)\n",
    "            print(f\"Features seleccionadas: {n_selected}/{n_total}\")\n",
    "            \n",
    "            # Mostrar cu√°les features se seleccionaron\n",
    "            print(f\"\\nFeatures seleccionadas:\")\n",
    "            selected_indices = np.where(gp_optimizer.feature_selection_)[0]\n",
    "            for idx in selected_indices:\n",
    "                if idx < X.shape[1]:\n",
    "                    print(f\"  X{idx} (original)\")\n",
    "                else:\n",
    "                    tree_idx = idx - X.shape[1]\n",
    "                    if tree_idx < len(gp_optimizer.best_trees_):\n",
    "                        print(f\"  {gp_optimizer.best_trees_[tree_idx].to_string()} (generada)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0751004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Con Optimizaci√≥n Evolutiva:\n",
      "  MAE: 0.5282\n",
      "  MSE: 0.5268\n",
      "  Features: 7\n",
      "\n",
      "======================================================================\n",
      "MEJORAS OBTENIDAS\n",
      "======================================================================\n",
      "Mejora en MAE: +0.94%\n",
      "Mejora en MSE: +5.22%\n"
     ]
    }
   ],
   "source": [
    "# Entrenar UN NUEVO MODELO con los datos optimizados (simula lo que hace el profesor)\n",
    "optimized_model = Ridge(alpha=1.0, random_state=42)\n",
    "optimized_model.fit(X_train_optimized, y_train)\n",
    "optimized_preds = optimized_model.predict(X_test_optimized)\n",
    "\n",
    "optimized_mae = mean_absolute_error(y_test, optimized_preds)\n",
    "optimized_mse = mean_squared_error(y_test, optimized_preds)\n",
    "\n",
    "\n",
    "print(f\"\\nCon Optimizaci√≥n Evolutiva:\")\n",
    "print(f\"  MAE: {optimized_mae:.4f}\")\n",
    "print(f\"  MSE: {optimized_mse:.4f}\")\n",
    "print(f\"  Features: {X_train_optimized.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular mejoras\n",
    "mae_improvement = ((baseline_mae - optimized_mae) / baseline_mae * 100)\n",
    "mse_improvement = ((baseline_mse - optimized_mse) / baseline_mse * 100)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEJORAS OBTENIDAS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mejora en MAE: {mae_improvement:+.2f}%\")\n",
    "print(f\"Mejora en MSE: {mse_improvement:+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0cfe3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Con Optimizaci√≥n Evolutiva LR:\n",
      "  MAE: 0.5282\n",
      "  MSE: 0.5268\n",
      "  Features: 7\n",
      "\n",
      "======================================================================\n",
      "MEJORAS OBTENIDAS\n",
      "======================================================================\n",
      "Mejora en MAE: +0.93%\n",
      "Mejora en MSE: +5.24%\n"
     ]
    }
   ],
   "source": [
    "# Entrenar UN NUEVO MODELO con los datos optimizados (simula lo que hace el profesor)\n",
    "baseline2 = LinearRegression()\n",
    "baseline2.fit(X_train, y_train)\n",
    "baseline_preds2 = baseline2.predict(X_test)\n",
    "\n",
    "baseline_mae2 = mean_absolute_error(y_test, baseline_preds2)\n",
    "baseline_mse2 = mean_squared_error(y_test, baseline_preds2)\n",
    "optimized_model2 = LinearRegression()\n",
    "optimized_model2.fit(X_train_optimized, y_train)\n",
    "optimized_preds2 = optimized_model2.predict(X_test_optimized)\n",
    "\n",
    "optimized_mae2 = mean_absolute_error(y_test, optimized_preds2)\n",
    "optimized_mse2 = mean_squared_error(y_test, optimized_preds2)\n",
    "\n",
    "print(f\"\\nCon Optimizaci√≥n Evolutiva LR:\")\n",
    "print(f\"  MAE: {optimized_mae2:.4f}\")\n",
    "print(f\"  MSE: {optimized_mse2:.4f}\")\n",
    "print(f\"  Features: {X_train_optimized.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular mejoras\n",
    "mae_improvement2 = ((baseline_mae2 - optimized_mae2) / baseline_mae2 * 100)\n",
    "mse_improvement2 = ((baseline_mse2 - optimized_mse2) / baseline_mse2 * 100)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEJORAS OBTENIDAS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mejora en MAE: {mae_improvement2:+.2f}%\")\n",
    "print(f\"Mejora en MSE: {mse_improvement2:+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d321e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE (Sin Optimizaci√≥n)\n",
      "======================================================================\n",
      "MAE: 0.3275\n",
      "MSE: 0.2554\n",
      "Features utilizadas: 8\n",
      "\n",
      "Con Optimizaci√≥n Evolutiva RF:\n",
      "  MAE: 0.3237\n",
      "  MSE: 0.2552\n",
      "  Features: 7\n",
      "\n",
      "======================================================================\n",
      "MEJORAS OBTENIDAS\n",
      "======================================================================\n",
      "Mejora en MAE: +1.17%\n",
      "Mejora en MSE: +0.08%\n"
     ]
    }
   ],
   "source": [
    "# Entrenar UN NUEVO MODELO con los datos optimizados (simula lo que hace el profesor)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "baseline2 = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "baseline2.fit(X_train, y_train)\n",
    "baseline_preds2 = baseline2.predict(X_test)\n",
    "\n",
    "baseline_mae2 = mean_absolute_error(y_test, baseline_preds2)\n",
    "baseline_mse2 = mean_squared_error(y_test, baseline_preds2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimizaci√≥n)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"MAE: {baseline_mae2:.4f}\")\n",
    "print(f\"MSE: {baseline_mse2:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "\n",
    "optimized_model2 = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "optimized_model2.fit(X_train_optimized, y_train)\n",
    "optimized_preds2 = optimized_model2.predict(X_test_optimized)\n",
    "\n",
    "optimized_mae2 = mean_absolute_error(y_test, optimized_preds2)\n",
    "optimized_mse2 = mean_squared_error(y_test, optimized_preds2)\n",
    "\n",
    "print(f\"\\nCon Optimizaci√≥n Evolutiva RF:\")\n",
    "print(f\"  MAE: {optimized_mae2:.4f}\")\n",
    "print(f\"  MSE: {optimized_mse2:.4f}\")\n",
    "print(f\"  Features: {X_train_optimized.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular mejoras\n",
    "mae_improvement2 = ((baseline_mae2 - optimized_mae2) / baseline_mae2 * 100)\n",
    "mse_improvement2 = ((baseline_mse2 - optimized_mse2) / baseline_mse2 * 100)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEJORAS OBTENIDAS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mejora en MAE: {mae_improvement2:+.2f}%\")\n",
    "print(f\"Mejora en MSE: {mse_improvement2:+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc469e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE (Sin Optimizaci√≥n)\n",
      "======================================================================\n",
      "MAE: 0.8600\n",
      "MSE: 1.3320\n",
      "Features utilizadas: 8\n",
      "\n",
      "Con Optimizaci√≥n Evolutiva SVR:\n",
      "  MAE: 0.4719\n",
      "  MSE: 0.4577\n",
      "  Features: 7\n",
      "\n",
      "======================================================================\n",
      "MEJORAS OBTENIDAS\n",
      "======================================================================\n",
      "Mejora en MAE: +45.12%\n",
      "Mejora en MSE: +65.64%\n"
     ]
    }
   ],
   "source": [
    "# Entrenar UN NUEVO MODELO con los datos optimizados (simula lo que hace el profesor)\n",
    "from sklearn.svm import SVR\n",
    "baseline2 = SVR()\n",
    "baseline2.fit(X_train, y_train)\n",
    "baseline_preds2 = baseline2.predict(X_test)\n",
    "\n",
    "baseline_mae2 = mean_absolute_error(y_test, baseline_preds2)\n",
    "baseline_mse2 = mean_squared_error(y_test, baseline_preds2)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimizaci√≥n)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"MAE: {baseline_mae2:.4f}\")\n",
    "print(f\"MSE: {baseline_mse2:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "optimized_model2 = SVR()\n",
    "optimized_model2.fit(X_train_optimized, y_train)\n",
    "optimized_preds2 = optimized_model2.predict(X_test_optimized)\n",
    "\n",
    "optimized_mae2 = mean_absolute_error(y_test, optimized_preds2)\n",
    "optimized_mse2 = mean_squared_error(y_test, optimized_preds2)\n",
    "\n",
    "print(f\"\\nCon Optimizaci√≥n Evolutiva SVR:\")\n",
    "print(f\"  MAE: {optimized_mae2:.4f}\")\n",
    "print(f\"  MSE: {optimized_mse2:.4f}\")\n",
    "print(f\"  Features: {X_train_optimized.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular mejoras\n",
    "mae_improvement2 = ((baseline_mae2 - optimized_mae2) / baseline_mae2 * 100)\n",
    "mse_improvement2 = ((baseline_mse2 - optimized_mse2) / baseline_mse2 * 100)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEJORAS OBTENIDAS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mejora en MAE: {mae_improvement2:+.2f}%\")\n",
    "print(f\"Mejora en MSE: {mse_improvement2:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a5dd0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE (Sin Optimizaci√≥n)\n",
      "======================================================================\n",
      "MAE: 0.3096\n",
      "MSE: 0.2226\n",
      "Features utilizadas: 8\n",
      "\n",
      "Con Optimizaci√≥n Evolutiva XGBoost:\n",
      "  MAE: 0.3145\n",
      "  MSE: 0.2324\n",
      "  Features: 7\n",
      "\n",
      "======================================================================\n",
      "MEJORAS OBTENIDAS\n",
      "======================================================================\n",
      "Mejora en MAE: -1.60%\n",
      "Mejora en MSE: -4.40%\n"
     ]
    }
   ],
   "source": [
    "# import XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "baseline2 = XGBRegressor(n_jobs=-1, random_state=42, verbosity=0)\n",
    "baseline2.fit(X_train, y_train)\n",
    "baseline_preds2 = baseline2.predict(X_test)\n",
    "\n",
    "baseline_mae2 = mean_absolute_error(y_test, baseline_preds2)\n",
    "baseline_mse2 = mean_squared_error(y_test, baseline_preds2)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimizaci√≥n)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"MAE: {baseline_mae2:.4f}\")\n",
    "print(f\"MSE: {baseline_mse2:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "optimized_model2 = XGBRegressor(n_jobs=-1, random_state=42, verbosity=0)\n",
    "optimized_model2.fit(X_train_optimized, y_train)\n",
    "optimized_preds2 = optimized_model2.predict(X_test_optimized)\n",
    "\n",
    "optimized_mae2 = mean_absolute_error(y_test, optimized_preds2)\n",
    "optimized_mse2 = mean_squared_error(y_test, optimized_preds2)\n",
    "\n",
    "print(f\"\\nCon Optimizaci√≥n Evolutiva XGBoost:\")\n",
    "print(f\"  MAE: {optimized_mae2:.4f}\")\n",
    "print(f\"  MSE: {optimized_mse2:.4f}\")\n",
    "print(f\"  Features: {X_train_optimized.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular mejoras\n",
    "mae_improvement2 = ((baseline_mae2 - optimized_mae2) / baseline_mae2 * 100)\n",
    "mse_improvement2 = ((baseline_mse2 - optimized_mse2) / baseline_mse2 * 100)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEJORAS OBTENIDAS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mejora en MAE: {mae_improvement2:+.2f}%\")\n",
    "print(f\"Mejora en MSE: {mse_improvement2:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE (Sin Optimizaci√≥n)\n",
      "======================================================================\n",
      "MAE: 0.3716\n",
      "MSE: 0.2940\n",
      "Features utilizadas: 8\n",
      "\n",
      "Con Optimizaci√≥n Evolutiva XGBoost:\n",
      "  MAE: 0.3627\n",
      "  MSE: 0.2884\n",
      "  Features: 7\n",
      "\n",
      "======================================================================\n",
      "MEJORAS OBTENIDAS\n",
      "======================================================================\n",
      "Mejora en MAE: +2.41%\n",
      "Mejora en MSE: +1.89%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import GradientBoosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "baseline2 = GradientBoostingRegressor(random_state=42)\n",
    "baseline2.fit(X_train, y_train)\n",
    "baseline_preds2 = baseline2.predict(X_test)\n",
    "\n",
    "baseline_mae2 = mean_absolute_error(y_test, baseline_preds2)\n",
    "baseline_mse2 = mean_squared_error(y_test, baseline_preds2)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BASELINE (Sin Optimizaci√≥n)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"MAE: {baseline_mae2:.4f}\")\n",
    "print(f\"MSE: {baseline_mse2:.4f}\")\n",
    "print(f\"Features utilizadas: {X_train.shape[1]}\")\n",
    "optimized_model2 = GradientBoostingRegressor(random_state=42)\n",
    "optimized_model2.fit(X_train_optimized, y_train)\n",
    "optimized_preds2 = optimized_model2.predict(X_test_optimized)\n",
    "\n",
    "optimized_mae2 = mean_absolute_error(y_test, optimized_preds2)\n",
    "optimized_mse2 = mean_squared_error(y_test, optimized_preds2)\n",
    "\n",
    "print(f\"\\nCon Optimizaci√≥n Evolutiva XGBoost:\")\n",
    "print(f\"  MAE: {optimized_mae2:.4f}\")\n",
    "print(f\"  MSE: {optimized_mse2:.4f}\")\n",
    "print(f\"  Features: {X_train_optimized.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular mejoras\n",
    "mae_improvement2 = ((baseline_mae2 - optimized_mae2) / baseline_mae2 * 100)\n",
    "mse_improvement2 = ((baseline_mse2 - optimized_mse2) / baseline_mse2 * 100)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MEJORAS OBTENIDAS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mejora en MAE: {mae_improvement2:+.2f}%\")\n",
    "print(f\"Mejora en MSE: {mse_improvement2:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58495a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROBANDO MODELOS ADICIONALES\n",
      "================================================================================\n",
      "\n",
      "Probando Lasso...\n",
      "  MAE: 0.7616 ‚Üí 0.9061 (-18.97%)\n",
      "  MSE: 0.9380 ‚Üí 1.3107 (-39.73%)\n",
      "\n",
      "Probando ElasticNet...\n",
      "  MAE: 0.6763 ‚Üí 0.8391 (-24.08%)\n",
      "  MSE: 0.7646 ‚Üí 1.1273 (-47.45%)\n",
      "\n",
      "Probando BayesianRidge...\n",
      "  MAE: 0.5332 ‚Üí 0.5282 (+0.94%)\n",
      "  MSE: 0.5556 ‚Üí 0.5268 (+5.18%)\n",
      "\n",
      "Probando HuberRegressor...\n",
      "  MAE: 0.5876 ‚Üí 0.4756 (+19.06%)\n",
      "  MSE: 0.7119 ‚Üí 0.4560 (+35.95%)\n",
      "\n",
      "Probando KNeighbors...\n",
      "  MAE: 0.8128 ‚Üí 0.4105 (+49.50%)\n",
      "  MSE: 1.1187 ‚Üí 0.3888 (+65.25%)\n",
      "\n",
      "Probando DecisionTree...\n",
      "  MAE: 0.4547 ‚Üí 0.4370 (+3.89%)\n",
      "  MSE: 0.4952 ‚Üí 0.4695 (+5.19%)\n",
      "\n",
      "Probando ExtraTrees...\n",
      "  MAE: 0.3270 ‚Üí 0.3225 (+1.37%)\n",
      "  MSE: 0.2539 ‚Üí 0.2527 (+0.49%)\n",
      "\n",
      "Probando AdaBoost...\n",
      "  MAE: 0.6498 ‚Üí 0.6399 (+1.51%)\n",
      "  MSE: 0.6145 ‚Üí 0.6045 (+1.62%)\n",
      "\n",
      "Probando Bagging...\n",
      "  MAE: 0.3279 ‚Üí 0.3237 (+1.28%)\n",
      "  MSE: 0.2559 ‚Üí 0.2548 (+0.42%)\n",
      "\n",
      "Probando MLP...\n",
      "  MAE: 0.6506 ‚Üí 0.4442 (+31.72%)\n",
      "  MSE: 0.7129 ‚Üí 0.4003 (+43.85%)\n",
      "\n",
      "Probando KernelRidge...\n"
     ]
    }
   ],
   "source": [
    "# Importar las funciones y librer√≠as adicionales\n",
    "from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import Lasso, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos adicionales para probar\n",
    "additional_models = {\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'Bagging': BaggingRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'MLP': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'KernelRidge': KernelRidge(alpha=1.0)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROBANDO MODELOS ADICIONALES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in additional_models.items():\n",
    "    try:\n",
    "        print(f\"\\nProbando {name}...\")\n",
    "        \n",
    "        # Baseline\n",
    "        model_baseline = clone(model)\n",
    "        model_baseline.fit(X_train, y_train)\n",
    "        baseline_preds = model_baseline.predict(X_test)\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "        baseline_mse = mean_squared_error(y_test, baseline_preds)\n",
    "        \n",
    "        # Con optimizaci√≥n\n",
    "        model_optimized = clone(model)\n",
    "        model_optimized.fit(X_train_optimized, y_train)\n",
    "        optimized_preds = model_optimized.predict(X_test_optimized)\n",
    "        \n",
    "        optimized_mae = mean_absolute_error(y_test, optimized_preds)\n",
    "        optimized_mse = mean_squared_error(y_test, optimized_preds)\n",
    "        \n",
    "        # Mejoras\n",
    "        mae_improvement = ((baseline_mae - optimized_mae) / baseline_mae * 100)\n",
    "        mse_improvement = ((baseline_mse - optimized_mse) / baseline_mse * 100)\n",
    "        \n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'MAE_Base': baseline_mae,\n",
    "            'MAE_Opt': optimized_mae,\n",
    "            'Mejora_MAE': mae_improvement,\n",
    "            'MSE_Base': baseline_mse,\n",
    "            'MSE_Opt': optimized_mse,\n",
    "            'Mejora_MSE': mse_improvement\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {baseline_mae:.4f} ‚Üí {optimized_mae:.4f} ({mae_improvement:+.2f}%)\")\n",
    "        print(f\"  MSE: {baseline_mse:.4f} ‚Üí {optimized_mse:.4f} ({mse_improvement:+.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"RESUMEN COMPLETO - TODOS LOS MODELOS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_sorted = df_results.sort_values('Mejora_MSE', ascending=False)\n",
    "\n",
    "print(f\"{'Modelo':<15} {'MAE Base':<10} {'MAE Opt':<10} {'Mejora MAE':<12} {'MSE Base':<10} {'MSE Opt':<10} {'Mejora MSE':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    print(f\"{row['Modelo']:<15} {row['MAE_Base']:<10.4f} {row['MAE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MAE']:+<12.2f}% {row['MSE_Base']:<10.4f} {row['MSE_Opt']:<10.4f} \"\n",
    "          f\"{row['Mejora_MSE']:+<12.2f}%\")\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTAD√çSTICAS GENERALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modelos que mejoraron MAE: {len(df_sorted[df_sorted['Mejora_MAE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Modelos que mejoraron MSE: {len(df_sorted[df_sorted['Mejora_MSE'] > 0])}/{len(df_sorted)}\")\n",
    "print(f\"Mejor mejora MAE: {df_sorted['Mejora_MAE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MAE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejor mejora MSE: {df_sorted['Mejora_MSE'].max():.2f}% ({df_sorted.loc[df_sorted['Mejora_MSE'].idxmax(), 'Modelo']})\")\n",
    "print(f\"Mejora promedio MAE: {df_sorted['Mejora_MAE'].mean():.2f}%\")\n",
    "print(f\"Mejora promedio MSE: {df_sorted['Mejora_MSE'].mean():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
